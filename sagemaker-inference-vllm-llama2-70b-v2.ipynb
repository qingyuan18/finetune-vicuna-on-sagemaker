{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ebed09",
   "metadata": {},
   "source": [
    "\n",
    "### Serve large models on SageMaker with DJL DeepSpeed Container\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container launched using from DeepSpeed and DJL. DJL provides for the serving framework while DeepSpeed is the key sharding library we leverage to enable hosting of large models.We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy the open source llama 7B model across GPU's on a ml.g5.48xlarge instance. Note that the llama 7B fp16 model can be deployed on single GPU such as g5.2xlarge (24GB VRAM), we jsut show the code which can deploy the llm accross multiple GPUs in SageMaker. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. For further reading on DeepSpeed you can refer to https://arxiv.org/pdf/2207.00032.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b16067",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. In this notebook we are going to create the model with the Inference code to shorten the end point creation time. \n",
    "\n",
    "The tarball is in the following format\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "│   └── serving.properties\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- `model.py` is the key file which will handle any requests for serving. \n",
    "- `requirements.txt` has the required libraries needed to be installed when the container starts up.\n",
    "- `serving.properties` is the script that will have environment variables which can be used to customize model.py at run time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ae21b-4508-4600-a4ad-8bd903052794",
   "metadata": {
    "tags": []
   },
   "source": [
    "### model download and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd74f7f-09c5-4dc9-8b30-a164be553fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vllm'...\n",
      "remote: Enumerating objects: 3042, done.\u001b[K\n",
      "remote: Counting objects: 100% (1374/1374), done.\u001b[K\n",
      "remote: Compressing objects: 100% (426/426), done.\u001b[K\n",
      "remote: Total 3042 (delta 1133), reused 1000 (delta 942), pack-reused 1668\u001b[K\n",
      "Receiving objects: 100% (3042/3042), 3.22 MiB | 28.68 MiB/s, done.\n",
      "Resolving deltas: 100% (2055/2055), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/vllm-project/vllm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d956f50-b14a-4ab5-adb4-0bfd07e07f62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.179.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.183.0.tar.gz (869 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.28.33)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.33)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.33->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.183.0-py2.py3-none-any.whl size=1185520 sha256=9b9f25c05c464292554a61a11b15a984c48312f24b68189a5cc620d8b0642baa\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ba/9a/74/751ecb146fb033eb48ab44784a576896fd86f9987a2863581f\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.179.0\n",
      "    Uninstalling sagemaker-2.179.0:\n",
      "      Successfully uninstalled sagemaker-2.179.0\n",
      "Successfully installed sagemaker-2.183.0\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub -Uqq\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b6766c-1ab2-49fd-b93a-e18a38558e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ab2890-6b33-4163-ba7e-d9c0e917a937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./LLM_llama2_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"TheBloke/Llama-2-70B-fp16\"\n",
    "commit_hash = \"e9a40d73f8b9c160cf11b04644f6d3fcecf46687\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84190de-4d68-4764-b257-3ac49af49915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acd96f912e74976950d706874aafb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 27 files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aee1e23ec0d48fcac8dbf621e738c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)6d3fcecf46687/Notice:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8144dbdfd674acca2f15d523f83f27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ecf46687/LICENSE.txt:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772122ae43c44bd6ae6b4429a5d1ecc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fcecf46687/README.md:   0%|          | 0.00/14.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b061b45bbf468d88c956c9fbb4775e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)f46687/USE_POLICY.md:   0%|          | 0.00/4.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c683752086df4fbfbdfc0465ca230774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3b3467cef34201abe56ad8a8a393b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ecf46687/config.json:   0%|          | 0.00/630 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973317ec069d48d2961bced07b37dfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)46687/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc93786692c84769a22f7623613b92ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4a9e732e9f47fbab34a5239375cf29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce272cc18e0b4bdfa0b93719a66ee185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36731daea57e498e952e90418558da69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a25d92df14244e58df8ad4e69ceb2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653fd6899913437187c640b968e1f174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8755492a8fa546d29e051bf9cd60f546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2e2c09c0a24c79b3de32e51ae5dd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f471c0f029f942e3b4cea2acb88b9767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9636b26421d747ab87ff6e2aca6f2a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46589938940c4a28b10f867af2e95f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb92a803a64433fb749f12f9da09218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219e649220e3469982efb66c233913d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c27b5f03444d4fa0c9d9e75175beaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/9.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4185b884494aff87c5f32962fda22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00015.safetensors:   0%|          | 0.00/524M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ebc0b7fc6447df98ebb5d92ab121a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/66.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cd495610b547a892c2ac27affa7e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab48f954365945e6a0b96207daa44f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)46687/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104adf2f3de64de3a668a2b579d86693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74add812e7047db92510eb460c21e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(repo_id=model_name, revision=commit_hash, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381d54f1-3033-413f-982b-dfc355fa7181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_code_prefix: LLM-RAG/workshop/LLM_llama2_sb_deploy_code\n",
      "model_snapshot_path: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687\n"
     ]
    }
   ],
   "source": [
    "s3_model_prefix = \"LLM-RAG/workshop/LLM_llama2_model\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = \"LLM-RAG/workshop/LLM_llama2_sb_deploy_code\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10d36b4-34aa-47a9-93cc-e3c245fda001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/README.md\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/.gitattributes\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/LICENSE.txt\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/Notice\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/config.json\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/USE_POLICY.md\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/generation_config.json\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00006-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00002-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00007-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00004-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00005-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00011-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00001-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00003-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00010-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00008-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00013-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model.safetensors.index.json\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/tokenizer.json\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/special_tokens_map.json\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/tokenizer.model\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00012-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/tokenizer_config.json\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00014-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00015-of-00015.safetensors\n",
      "delete: s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00009-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/Notice to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/Notice\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/LICENSE.txt to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/LICENSE.txt\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/generation_config.json to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/generation_config.json\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/config.json to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/config.json\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/USE_POLICY.md to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/USE_POLICY.md\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/.gitattributes to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/.gitattributes\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/README.md to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/README.md\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00005-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00005-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00004-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00004-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00002-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00002-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00001-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00001-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00003-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00003-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00006-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00006-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00009-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00009-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00008-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00008-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00010-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00010-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00007-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00007-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00015-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00015-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model.safetensors.index.json to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model.safetensors.index.json\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/special_tokens_map.json to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/special_tokens_map.json\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/tokenizer.json to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/tokenizer.json\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/tokenizer.model to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/tokenizer.model\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/tokenizer_config.json to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/tokenizer_config.json\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00014-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00014-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00012-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00012-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00013-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00013-of-00015.safetensors\n",
      "upload: LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687/model-00011-of-00015.safetensors to s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/model-00011-of-00015.safetensors\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://{bucket}/{s3_model_prefix}\n",
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27bbdc0-0f5b-4fcd-a1d3-bf98bae27d47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_model_location => s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/\n"
     ]
    }
   ],
   "source": [
    "s3_model_location = f\"s3://{bucket}/{s3_model_prefix}/\"\n",
    "print(\"s3_model_location => {}\".format(s3_model_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a365be6-d9c0-4dcc-8a1e-4e2d8a6ab2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-06 07:24:04       1519 .gitattributes\n",
      "2023-09-06 07:24:04       7020 LICENSE.txt\n",
      "2023-09-06 07:24:04        112 Notice\n",
      "2023-09-06 07:24:04      14044 README.md\n",
      "2023-09-06 07:24:04       4766 USE_POLICY.md\n",
      "2023-09-06 07:24:04        630 config.json\n",
      "2023-09-06 07:24:04        179 generation_config.json\n",
      "2023-09-06 07:24:04 9852591960 model-00001-of-00015.safetensors\n",
      "2023-09-06 07:24:04 9798099016 model-00002-of-00015.safetensors\n",
      "2023-09-06 07:24:04 9965870512 model-00003-of-00015.safetensors\n",
      "2023-09-06 07:24:04 9798066064 model-00004-of-00015.safetensors\n",
      "2023-09-06 07:24:04 9798099064 model-00005-of-00015.safetensors\n",
      "2023-09-06 07:27:45 9798099056 model-00006-of-00015.safetensors\n",
      "2023-09-06 07:27:46 9965870512 model-00007-of-00015.safetensors\n",
      "2023-09-06 07:27:46 9798066064 model-00008-of-00015.safetensors\n",
      "2023-09-06 07:27:47 9798099064 model-00009-of-00015.safetensors\n",
      "2023-09-06 07:27:50 9798099056 model-00010-of-00015.safetensors\n",
      "2023-09-06 07:31:33 9965870512 model-00011-of-00015.safetensors\n",
      "2023-09-06 07:31:35 9798066064 model-00012-of-00015.safetensors\n",
      "2023-09-06 07:31:37 9798099064 model-00013-of-00015.safetensors\n",
      "2023-09-06 07:31:41 9496124816 model-00014-of-00015.safetensors\n",
      "2023-09-06 07:31:41  524288128 model-00015-of-00015.safetensors\n",
      "2023-09-06 07:31:55      66725 model.safetensors.index.json\n",
      "2023-09-06 07:31:55        411 special_tokens_map.json\n",
      "2023-09-06 07:31:55    1842767 tokenizer.json\n",
      "2023-09-06 07:31:55     499723 tokenizer.model\n",
      "2023-09-06 07:31:55        749 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5796c856-f1b4-447a-86a9-5f5d84c7cc02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## model deployment locally(not need right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2c5fb6-c84d-4172-a74e-ab62088f793f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm\n",
      "  Obtaining dependency information for vllm from https://files.pythonhosted.org/packages/51/25/169816837b6bd3a0657e55ac35196add9376f6c8f059e4d9bf080ef8283d/vllm-0.1.4-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading vllm-0.1.4-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from vllm) (5.9.5)\n",
      "Collecting ray>=2.5.1 (from vllm)\n",
      "  Obtaining dependency information for ray>=2.5.1 from https://files.pythonhosted.org/packages/d5/d1/50c97d98a8ead5660b832136b7cfcab0b4ed02099bf0fa6e7b18beaf3ba8/ray-2.6.3-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ray-2.6.3-cp310-cp310-manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from vllm) (1.24.4)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from vllm) (2.0.1)\n",
      "Collecting transformers>=4.31.0 (from vllm)\n",
      "  Obtaining dependency information for transformers>=4.31.0 from https://files.pythonhosted.org/packages/e1/9d/4d9fe5c3b820db10773392ac5f4a0c8dab668f70b245ce2ce09785166128/transformers-4.33.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xformers>=0.0.21 (from vllm)\n",
      "  Obtaining dependency information for xformers>=0.0.21 from https://files.pythonhosted.org/packages/c7/b4/9f8bea4204f8482c9c9c64bcf86bd209ccfb2ebdb27e3590ef4c1e87b743/xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting fastapi (from vllm)\n",
      "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/76/e5/ca411b260caa4e72f9ac5482f331fe74fd4eb5b97aa74d1d2806ccf07e2c/fastapi-0.103.1-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.103.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn (from vllm)\n",
      "  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pydantic<2 (from vllm)\n",
      "  Obtaining dependency information for pydantic<2 from https://files.pythonhosted.org/packages/bc/e0/0371e9b6c910afe502e5fe18cc94562bfd9399617c7b4f5b6e13c29115b3/pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<2->vllm) (4.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray>=2.5.1->vllm) (8.1.6)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray>=2.5.1->vllm) (3.12.2)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray>=2.5.1->vllm) (4.18.4)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.5.1->vllm)\n",
      "  Downloading msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray>=2.5.1->vllm) (21.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray>=2.5.1->vllm) (4.23.4)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray>=2.5.1->vllm) (6.0)\n",
      "Collecting aiosignal (from ray>=2.5.1->vllm)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist (from ray>=2.5.1->vllm)\n",
      "  Obtaining dependency information for frozenlist from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray>=2.5.1->vllm) (2.31.0)\n",
      "Collecting grpcio>=1.42.0 (from ray>=2.5.1->vllm)\n",
      "  Obtaining dependency information for grpcio>=1.42.0 from https://files.pythonhosted.org/packages/28/fa/c38a010d3fffcac07ef121abb34eb2c3db0876df74267ce5bde13c3a6ed7/grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->vllm) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->vllm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->vllm) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->vllm) (0.16.4)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.31.0->vllm)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.31.0->vllm)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.31.0->vllm)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->vllm) (4.65.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi->vllm) (3.7.1)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->vllm)\n",
      "  Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->vllm)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->vllm) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->vllm) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->vllm) (1.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.31.0->vllm) (2023.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->ray>=2.5.1->vllm) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->vllm) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->ray>=2.5.1->vllm) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->ray>=2.5.1->vllm) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->ray>=2.5.1->vllm) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->ray>=2.5.1->vllm) (0.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ray>=2.5.1->vllm) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ray>=2.5.1->vllm) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ray>=2.5.1->vllm) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=2.0.0->vllm) (1.3.0)\n",
      "Downloading vllm-0.1.4-cp310-cp310-manylinux1_x86_64.whl (18.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.6.3-cp310-cp310-manylinux2014_x86_64.whl (56.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, sentencepiece, safetensors, ninja, msgpack, regex, pydantic, h11, grpcio, frozenlist, uvicorn, starlette, aiosignal, xformers, transformers, fastapi, ray, vllm\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.0.3\n",
      "    Uninstalling pydantic-2.0.3:\n",
      "      Successfully uninstalled pydantic-2.0.3\n",
      "Successfully installed aiosignal-1.3.1 fastapi-0.103.1 frozenlist-1.4.0 grpcio-1.57.0 h11-0.14.0 msgpack-1.0.5 ninja-1.11.1 pydantic-1.10.12 ray-2.6.3 regex-2023.8.8 safetensors-0.3.3 sentencepiece-0.1.99 starlette-0.27.0 tokenizers-0.13.3 transformers-4.33.0 uvicorn-0.23.2 vllm-0.1.4 xformers-0.0.21\n",
      "Requirement already satisfied: ray[air] in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.6.3)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (8.1.6)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (3.12.2)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (4.18.4)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (1.0.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (21.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (4.23.4)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (6.0)\n",
      "Requirement already satisfied: aiosignal in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (1.4.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (2.31.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (1.57.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (1.24.4)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (0.17.1)\n",
      "Requirement already satisfied: pydantic<2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (1.10.12)\n",
      "Requirement already satisfied: pandas>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (1.5.3)\n",
      "Collecting opencensus (from ray[air])\n",
      "  Downloading opencensus-0.11.2-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: uvicorn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (0.23.2)\n",
      "Collecting smart-open (from ray[air])\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp>=3.7 (from ray[air])\n",
      "  Obtaining dependency information for aiohttp>=3.7 from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting colorful (from ray[air])\n",
      "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gpustat>=1.0.0 (from ray[air])\n",
      "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (2023.6.0)\n",
      "Collecting py-spy>=0.2.0 (from ray[air])\n",
      "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (12.0.1)\n",
      "Requirement already satisfied: starlette in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (0.27.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray[air]) (0.103.1)\n",
      "Collecting tensorboardX>=1.9 (from ray[air])\n",
      "  Obtaining dependency information for tensorboardX>=1.9 from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aiohttp-cors (from ray[air])\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting virtualenv<20.21.1,>=20.0.24 (from ray[air])\n",
      "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiorwlock (from ray[air])\n",
      "  Downloading aiorwlock-1.3.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp>=3.7->ray[air]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp>=3.7->ray[air]) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.7->ray[air])\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp>=3.7->ray[air])\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.7->ray[air])\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-ml-py>=11.450.129 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[air]) (12.535.77)\n",
      "Requirement already satisfied: psutil>=5.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[air]) (5.9.5)\n",
      "Collecting blessed>=1.17.1 (from gpustat>=1.0.0->ray[air])\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.3->ray[air]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.3->ray[air]) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<2->ray[air]) (4.7.1)\n",
      "Collecting distlib<1,>=0.3.6 (from virtualenv<20.21.1,>=20.0.24->ray[air])\n",
      "  Obtaining dependency information for distlib<1,>=0.3.6 from https://files.pythonhosted.org/packages/43/a0/9ba967fdbd55293bacfc1507f58e316f740a3b231fc00e3d86dc39bc185a/distlib-0.3.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading distlib-0.3.7-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[air]) (3.9.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi->ray[air]) (3.7.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->ray[air]) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->ray[air]) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->ray[air]) (0.9.2)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus->ray[air])\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[air])\n",
      "  Obtaining dependency information for google-api-core<3.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/6e/c4/c3cd048b6cbeba8d9ae50dd7643ac065b85237338aa7501b0efae91eb4d9/google_api_core-2.11.1-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->ray[air]) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ray[air]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ray[air]) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ray[air]) (2023.5.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn->ray[air]) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->ray[air]) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->ray[air]) (1.1.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (0.2.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (1.16.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air])\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0.dev0,>=1.56.2 from https://files.pythonhosted.org/packages/a7/bc/416a1ffeba4dcd072bc10523dac9ed97f2e7fc4b760580e2bdbdc1e2afdd/googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air])\n",
      "  Obtaining dependency information for google-auth<3.0.dev0,>=2.14.1 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air])\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air])\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (4.7.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.4.8)\n",
      "Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Building wheels for collected packages: gpustat\n",
      "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26411 sha256=58bce3bae2b23d3159ca9557d505ec1e5c02e8c5186e71e3fd132412cfd4d672\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ec/d7/80/a71ba3540900e1f276bcae685efd8e590c810d2108b95f1e47\n",
      "Successfully built gpustat\n",
      "Installing collected packages: py-spy, opencensus-context, distlib, colorful, virtualenv, smart-open, pyasn1-modules, multidict, googleapis-common-protos, cachetools, blessed, async-timeout, aiorwlock, yarl, tensorboardX, gpustat, google-auth, google-api-core, aiohttp, opencensus, aiohttp-cors\n",
      "Successfully installed aiohttp-3.8.5 aiohttp-cors-0.7.0 aiorwlock-1.3.0 async-timeout-4.0.3 blessed-1.20.0 cachetools-5.3.1 colorful-0.5.5 distlib-0.3.7 google-api-core-2.11.1 google-auth-2.22.0 googleapis-common-protos-1.60.0 gpustat-1.1.1 multidict-6.0.4 opencensus-0.11.2 opencensus-context-0.1.3 py-spy-0.3.14 pyasn1-modules-0.3.0 smart-open-6.3.0 tensorboardX-2.6.2.2 virtualenv-20.21.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install vllm\n",
    "!pip3 install accelerate>=0.20.3\n",
    "!pip3 install ray[air]\n",
    "!pip3 install transformers>=4.32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb23582-e43c-4039-90cc-3032532d278d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 08:01:36,218\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "def get_model():\n",
    "    model_location = \"./LLM_llama2_model/models--TheBloke--Llama-2-70B-fp16/snapshots/e9a40d73f8b9c160cf11b04644f6d3fcecf46687\"\n",
    "    tensor_parallel_degree = 8\n",
    "\n",
    "    llm = LLM(model=model_location, tensor_parallel_size=int(tensor_parallel_degree))\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_location, torch_dtype=torch.float16)\n",
    "    return llm,tokenizer\n",
    "\n",
    "\n",
    "def handle(data) -> None:\n",
    "    global predictor\n",
    "    global tokenizer\n",
    "    if not predictor:\n",
    "        predictor,tokenizer = get_model()\n",
    "\n",
    "        \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    params = data.get(\"params\",{})\n",
    "    \n",
    "        \n",
    "    sampling_params = SamplingParams(temperature=params[\"temperature\"], top_p=params[\"top_p\"], max_tokens=params[\"max_tokens\"])\n",
    "    result = predictor.generate(data[\"inputs\"], sampling_params)\n",
    "    \n",
    "    result_json = []\n",
    "    for output in result:\n",
    "        prompt = output.prompt\n",
    "        input_tokens = tokenizer.tokenize(prompt)\n",
    "        input_token_lenth = len(input_tokens)\n",
    "        print(f\"input token lenth:{input_token_lenth}\")\n",
    "        \n",
    "        generated_text = output.outputs[0].text\n",
    "        output_tokens = tokenizer.tokenize(generated_text)\n",
    "        output_token_lenth = len(output_tokens)\n",
    "        print(f\"output token lenth:{output_token_lenth}\")\n",
    "        \n",
    "        result_json.append(generated_text)\n",
    "    print(result_json)\n",
    "\n",
    "prompt=\"##Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.#### Malcolm:Oh. What are you wearing right now, pet?## Eva:\"\n",
    "prompt=\"a happy weekend with my family, I\"\n",
    "parameters = {\n",
    "  \"early_stopping\": True,\n",
    "  \"max_tokens\": 300,\n",
    "  \"min_new_tokens\": 128,\n",
    "  \"top_p\":0.95\n",
    "  #\"do_sample\": False,\n",
    "  #\"temperature\": 1.0,\n",
    "}\n",
    "data = {\n",
    "    \"inputs\":[prompt],\n",
    "    \"params\": parameters\n",
    "}\n",
    "dataJson = json.dumps(data)\n",
    "model,tokenizer = get_model()\n",
    "handle(dataJson)\n",
    "\n",
    "end_time = time.time()\n",
    "time_interval = end_time - start_time\n",
    "print(f\"代码执行时间间隔（秒）：{time_interval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a40ba8-5a56-41d7-88a3-99a97126075f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### model deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b846e",
   "metadata": {},
   "source": [
    "#### Serving.properties has engine parameter which tells the DJL model server to use the DeepSpeed engine to load the model.\n",
    "\n",
    "option.tensor_parallel_degree:  now we use the g5.48xlarge which has 8 GPUs, so we set the tensor_parallel_degree to 8.\n",
    "\n",
    "option.s3url:  you should use your model path here. And the s3 path must be ended with \"/\".\n",
    "\n",
    "batch_size:   it is for server side batch based on request level. You can set batch_size to the large value which can not result in the OOM. The current code about model.py is just demo for one prompt per client request.\n",
    "\n",
    "max_batch_delay:   it is counted by millisecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfe206-b728-4692-91c0-eb81b1109c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf src\n",
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d4447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=8\n",
    "#option.model_id=THUDM/llama2-6b\n",
    "option.s3url=s3://sagemaker-us-west-2-687912291502/LLM-RAG/workshop/LLM_llama2_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e7f4f-04ed-4602-a6d4-26c6f6dc3482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "vllm\n",
    "accelerate>=0.20.3\n",
    "ray[air]\n",
    "transformers>=4.32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb61a24-90dc-4184-8880-3f7c68f84218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/model.py\n",
    "from vllm import LLM, SamplingParams\n",
    "from djl_python import Input, Output\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "\n",
    "predictor = None\n",
    "tokenizer = None\n",
    "\n",
    "def get_model(properties):\n",
    "    model_location = properties['model_dir']\n",
    "    tensor_parallel_degree = properties[\"tensor_parallel_degree\"]\n",
    "    \n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "\n",
    "    llm = LLM(model=model_location, tensor_parallel_size=int(tensor_parallel_degree))\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_location, torch_dtype=torch.float16)\n",
    "    return llm,tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor\n",
    "    global tokenizer\n",
    "    if not predictor:\n",
    "        predictor,tokenizer = get_model(inputs.get_properties())\n",
    "\n",
    "        \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json()\n",
    "    params = data.get(\"params\",{})\n",
    "    \n",
    "        \n",
    "    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=params[\"max_tokens\"])\n",
    "    result = predictor.generate(data[\"inputs\"], sampling_params)\n",
    "    \n",
    "    result_json = []\n",
    "    for output in result:\n",
    "        prompt = output.prompt\n",
    "        #input_tokens = tokenizer.tokenize(prompt)\n",
    "        #input_token_lenth = len(input_tokens)\n",
    "        #print(f\"input token lenth:{input_token_lenth}\")\n",
    "        \n",
    "        generated_text = output.outputs[0].text\n",
    "        #output_tokens = tokenizer.tokenize(generated_text)\n",
    "        #output_token_lenth = len(output_tokens)\n",
    "        #print(f\"output token lenth:{output_token_lenth}\")\n",
    "        \n",
    "        result_json.append(generated_text)\n",
    "    return Output().add(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c5a8b",
   "metadata": {},
   "source": [
    "#### Create required variables and initialize them to create the endpoint, we leverage boto3 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870dc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sage_session = sagemaker.Session()\n",
    "model_bucket = sage_session.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    #\"hf-large-model-llama-7b-0625/code\"  # folder within bucket where code artifact will go\n",
    "    \"llama2-vllm/code\"\n",
    ")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92b3ee",
   "metadata": {},
   "source": [
    "**Image URI for the DJL container is being used here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e39d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Note that: you can modify the image url according to your specific region.\n",
    "#inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    "#print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "inference_image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\" \n",
    "#inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f18061",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3c330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15362c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sage_session.upload_data(\"model.tar.gz\", model_bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68348e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"S3 Model Bucket is -- > {model_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a688d33",
   "metadata": {},
   "source": [
    "### To create the end point the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.g5.48xlarge \n",
    "    \n",
    "    b) ContainerStartupHealthCheckTimeoutInSeconds is 15*60 to ensure health check starts after the model is ready\n",
    "    \n",
    "3. Create the end point using the endpoint config created    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dde408",
   "metadata": {},
   "source": [
    "One of the key parameters here is **TENSOR_PARALLEL_DEGREE** which essentially tells the DeepSpeed library to partition the models along 8 GPU's. This is a tunable and configurable parameter.\n",
    "\n",
    "This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests. For further reading on DeepSpeedyou can follow the link https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc711bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"llama2-70b-vllm\")\n",
    "print(model_name)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ca36f",
   "metadata": {},
   "source": [
    "VolumnSizeInGB has been left as commented out. You should use this value for Instance types which support EBS volume mounts. The current instance we are using comes with a pre configured space and does not support additional volume mounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858198d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config-0902\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.48xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            #\"VolumeSizeInGB\" : 300,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 15*60,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61741f2-7416-4a65-8531-30cd379e3dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e73d7a",
   "metadata": {},
   "source": [
    "#### Wait for the end point to be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da06c31",
   "metadata": {},
   "source": [
    "### This step can take ~ 15 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0982c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254baa8",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will complete the sentence and return the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609ca7c-ca07-446a-986f-7555ef20a591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "prompt1 = \"\"\"根据以下反引号内的商品详细描述，为电商直播主持人创作一段引人注目的商品推介话术\n",
    "‘’‘\n",
    "iPhone 14是苹果公司在2022年9月8日正式发布的最新手机。它配备了一块6.1英寸的OLED屏幕，并提供了六种独特的颜色选择：蓝色、紫色、午夜色、星光色、红色和黄色。手机的尺寸设计优雅，长度为146.7毫米，宽度为71.5毫米，厚度为7.8毫米，重量约为172克。\n",
    "在性能上，iPhone 14搭载了强大的苹果A15仿生芯片，内部含有6 核中央处理器，有 2 个性能核心和 4 个能效核心，还有5 核GPU图形处理器。。它不仅支持车祸检测和卫星通信等实用功能，而且在拍照方面也表现出色。\n",
    "后置摄像头包括一个1200万像素的主镜头和一个1200万像素的超广角镜头，前置摄像头也是1200万像素\n",
    "此外，该手机还支持光像引擎、深度融合技术、智能HDR4和人像模式等摄影技术，确保用户可以轻松捕捉每一个美好瞬间\n",
    "’‘’\n",
    "话术中应包括商品的主要特点、优势及互动环节,使用中文撰写，并保持话术简洁、有趣且具吸引力,并确保包含上述要求的所有元素\"\"\"\n",
    "\n",
    "prompt2=\"\"\"根据以下反引号内的关键词，为电商直播主持人创作一段通用的开场、互动或欢迎话术。请确保话术融入这些关键词，使用中文撰写，内容要简洁、有趣且具吸引力，同时适应广泛的商品和场景。\n",
    "‘’‘\n",
    "精选\n",
    "性价比\n",
    "品质\n",
    "日常家居\n",
    "穿搭\n",
    "限时折扣\n",
    "免费赠品\n",
    "抽奖活动\n",
    "大品牌合作\n",
    "独家优惠\n",
    "’‘’\n",
    "请使用上述关键词，编写一段具有普遍适用性，适于电商直播开头或互动环节的话术\"\"\"\n",
    "\n",
    "\n",
    "prompt3=\"\"\"\n",
    "请根据以下反引号内的商品描述、意图、问题模板和回答模板，为电商直播商品生成一个问答库。要求生成的回答应当有至少一组，最多五组。请确保答案基于商品描述和回答模板生成。如果无法生成回答，表示为“根据已知信息无法生成回答”。格式应如下：{{\"Q\":\"问题\",\"A\":['答案1-1','答案1-2'...]}}\n",
    "‘’‘\n",
    "商品描述：iPhone 14是苹果公司在2022年9月8日正式发布的最新手机。它配备了一块6.1英寸的OLED屏幕，并提供了六种独特的颜色选择：蓝色、紫色、午夜色、星光色、红色和黄色。手机的尺寸设计优雅，长度为146.7毫米，宽度为71.5毫米，厚度为7.8毫米，重量约为172克。\n",
    "在性能上，iPhone 14搭载了强大的苹果A15仿生芯片，内部含有6 核中央处理器，有 2 个性能核心和 4 个能效核心，还有5 核GPU图形处理器。。它不仅支持车祸检测和卫星通信等实用功能，而且在拍照方面也表现出色。后置摄像头包括一个1200万像素的主镜头和一个1200万像素的超广角镜头，前置摄像头也是1200万像素。此外，该手机还支持光像引擎、深度融合技术、智能HDR4和人像模式等摄影技术，确保用户可以轻松捕捉每一个美好瞬间。}\n",
    "意图：性能\n",
    "问题模板：手机的性能如何？\n",
    "回答模板：[商品名称]采用了最新的[芯片名称]，搭载了[核心数量]核CPU和[GPU核心数量]核GPU，为用户提供强大的性能。\n",
    "谈到[商品名称]的性能，不得不提及它的[芯片名称]，配备[核心数量]核CPU和[GPU核心数量]核GPU，应对各种任务都游刃有余。\n",
    "[商品名称]在性能上表现卓越，得益于其[芯片名称]和[核心数量]核处理器，加上[GPU核心数量]核GPU，让每次使用都顺畅无比。}\n",
    "’‘’\n",
    "问答生成：请基于上述商品描述、意图、问题模板和回答模板，为电商直播商品提供符合上述格式的问答库。\n",
    "\"\"\"\n",
    "\n",
    "prompt4=\"\"\"\n",
    "请以电商直播主持人的第一人称角度回答观众的商品相关问题。确保只回答与商品相关的问题，并只使用以下反引号内知识库的信息来回答。回答中请勿随意编造内容。格式应如下:[{{\"intention\": \"意图1\", \"answer\": \"回答1\"}},{{\"intention\": \"意图2\", \"answer\": \"回答2\"}}]\n",
    "‘’‘\n",
    "[问题：iPhone 14有哪些可选的颜色？][回答：iPhone 14提供了六种时尚的颜色选择，包括蓝色、紫色、午夜色、星光色、红色和黄色。][意图：颜色]\n",
    "[问题：关于摄像头，iPhone 14的前置和后置摄像头分辨率是多少？][回答：iPhone 14的前置和后置摄像头分辨率都是1200万像素。][意图：分辨率]\n",
    "[问题：我经常用手机办公和玩游戏，iPhone 14的性能如何？][回答：iPhone 14搭载了强大的苹果A15六核中央处理器，无论是玩游戏、看视频，还是办公，它都可以轻松应对。][意图：性能]}\n",
    "’‘’\n",
    "观众问题：主播小姐姐好漂亮\n",
    "使用第一人称直接回答观众关于商品的提问。检查知识库中是否有与观众提问相匹配的回答。对于在知识库中找到的每个匹配意图，请依次提供对应的回答，并确保从知识库中的意图中提取相应的意图标签。如果所有的意图都在知识库中找不到答案，回答“根据已知信息无法回答问题”。确保不使用emoji。\n",
    "\"\"\"\n",
    "\n",
    "other=\"\"\"我很在意手机的颜色和摄像头功能，能给我介绍一下iPhone 14在这两方面的特点吗？\n",
    "便宜点就好了\"\"\"\n",
    "\n",
    "\n",
    "prompt_prefix = \"你正在一个聊天室里和不同国家的人们聊天，你能读懂所有国家的语言，你负责通过聊天记录分析所有聊天者的性格和有效信息，具体步骤如下：\\\n",
    "1.阅读他们的聊天记录 \\\n",
    "2.总结他们聊天里面的重要信息 \\\n",
    "3.抽象他们的人设 \\\n",
    "4.使用评分体系抽象他们之间的人际关系，然后给一个评分，范围1-10分，分越高关系越好 \\\n",
    "聊天信息如下: \" \n",
    "\n",
    "chats_infos = \"\"\"\n",
    "WaRGazmo : \"you lucked out there buddy\" \n",
    "WarLord : \"suerte? eso no existe \" \n",
    "WarLord : \"soy más rápido que la luz \" \n",
    "WaRGazmo : \"it exists.. or karma\" \n",
    "DirtyE1bow : \"so you was a planned birth ?\" \n",
    "WaRGazmo : \"thats what she said bruh\" \n",
    "WarLord : \"te amo mi amor \" \n",
    "Manowarik : \"Мир вам,люди добрые..\" \n",
    "kotofei : \"и тебе боярин, что не подался в челядь королю)\" \n",
    "XxNORxXMithra : \"God morgen folkens :) \" \n",
    "kotofei : \"и прочие жители галактики \" \n",
    "XxNORxXMithra : \"Ja de også forsåvidt :) \" \n",
    "Manowarik : \"Котофей-это который по цепи кругом?Песни там,сказки?😆😆\" \n",
    "kotofei : \"не, то дальний убогий родственник \" \n",
    "Manowarik : \"Эххх..Лукоморье мимо..((\" \n",
    "kipl : \"Котофей он из сказки Лиса и Котофей Иванович. \" \n",
    "kipl : \"Межвидовой брак и крышевание леса\" \n",
    "kotofei : \"лиса 🦊 мералиса и Котофей Иваныч \" \n",
    "leister : \"😆\" \n",
    "XxFoxyQBAxX : \"po co tyle zrobiłeś?\"\n",
    "\"\"\"\n",
    "\n",
    "prompt=\"##Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.#### Malcolm:Oh. What are you wearing right now, pet?## Eva:\"\n",
    "prompt=\"a happy weekend with my family, I\"\n",
    "parameters = {\n",
    "  \"early_stopping\": True,\n",
    "  \"max_tokens\": 300,\n",
    "  \"min_new_tokens\": 128,\n",
    "  #\"do_sample\": False,\n",
    "  #\"temperature\": 1.0,\n",
    "}\n",
    "\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": [prompt],\n",
    "                #\"inputs\": [prompt1,prompt3,prompt2,prompt2,prompt2,prompt2,prompt4],\n",
    "                \"params\": parameters\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "end_time = time.time()\n",
    "time_interval = end_time - start_time\n",
    "print(f\"代码执行时间间隔（秒）：{time_interval}\")\n",
    "\n",
    "response_model['Body'].read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3faa18-9c01-4f21-a9a5-5ba16f5958f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

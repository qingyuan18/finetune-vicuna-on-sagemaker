{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Distributed Training Demo\n",
    "\n",
    "### Model Parallelism using `SageMakerTrainer` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end distributed Text-Classification example. In this demo, we will use the Hugging Face `transformers` and `datasets` library together with a Amazon sagemaker-sdk extension to run GLUE `mnli` benchmark on a multi-node multi-gpu cluster using [SageMaker Model Parallelism Library](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html). The demo will use the new smdistributed library to run training on multiple gpus. We extended the `Trainer` API to a the `SageMakerTrainer` to use the model parallelism library. Therefore you only have to change the imports in your `train.py`.\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.7/site-packages (2.143.0)\n",
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.144.0.tar.gz (712 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m712.8/712.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.26.105)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.21.6)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.20.3)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Collecting importlib-metadata<5.0,>=1.4.0\n",
      "  Using cached importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (20.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.3.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.7.5)\n",
      "Collecting PyYAML==5.4.1\n",
      "  Using cached PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.2.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.48.0) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.105 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.48.0) (1.29.105)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.48.0) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.48.0) (3.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.48.0) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.48.0) (1.14.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.48.0) (0.15.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.48.0) (59.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.48.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.48.0) (2.8.2)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.6)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker>=2.48.0) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.105->boto3<2.0,>=1.26.28->sagemaker>=2.48.0) (1.26.15)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.144.0-py2.py3-none-any.whl size=958086 sha256=b5b4869136dbb5ab1f5d1a15b84553153838fff2b0452a8976e0ab846df3aab9\n",
      "  Stored in directory: /root/.cache/pip/wheels/07/b6/ac/c8fd0c283eb5375b8f4b23643985018319a9388bd185db4acb\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: PyYAML, importlib-metadata, sagemaker\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.1.0\n",
      "    Uninstalling importlib-metadata-6.1.0:\n",
      "      Successfully uninstalled importlib-metadata-6.1.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.143.0\n",
      "    Uninstalling sagemaker-2.143.0:\n",
      "      Successfully uninstalled sagemaker-2.143.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "awscli 1.27.105 requires rsa<4.8,>=3.1.2, but you have rsa 4.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-5.4.1 importlib-metadata-4.13.0 sagemaker-2.144.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::514385905925:role/service-role/AmazonSageMaker-ExecutionRole-20201218T184365\n",
      "sagemaker bucket: sagemaker-us-east-1-514385905925\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job\n",
    "\n",
    "In this example we are going to use the `run_glue.py` from the transformers example scripts. We modified it and included `SageMakerTrainer` instead of the `Trainer` to enable model-parallelism. You can find the code [here](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification).\n",
    "\n",
    "```python\n",
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments, SageMakerTrainer as Trainer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmetric_definitions=[\\n     {\\'Name\\': \\'train_runtime\\', \\'Regex\\':\"train_runtime.*=\\\\D*(.*?)$\"},\\n     {\\'Name\\': \\'train_samples_per_second\\', \\'Regex\\': \"train_samples_per_second.*=\\\\D*(.*?)$\"},\\n     {\\'Name\\': \\'epoch\\', \\'Regex\\': \"epoch.*=\\\\D*(.*?)$\"},\\n     {\\'Name\\': \\'f1\\', \\'Regex\\': \"f1.*=\\\\D*(.*?)$\"},\\n     {\\'Name\\': \\'exact_match\\', \\'Regex\\': \"exact_match.*=\\\\D*(.*?)$\"}]\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "#hyperparameters for flan-t5-xxl\n",
    "'''\n",
    "hyperparameters={\n",
    "    'train_dataset_path': '/opt/ml/input/data/training',\n",
    "    'test_dataset_path': '/opt/ml/input/data/test',\n",
    "    \"learning_rate\": 1e-4,\n",
    "}\n",
    "'''\n",
    "\n",
    "model_id=\"decapoda-research/llama-7b-hf\"\n",
    "# hyperparameters, which are passed into the training job\n",
    "#hyperparameters for llama\n",
    "hyperparameters={\n",
    "  'model_name': model_id,                                # pre-trained model\n",
    "  'training_dir': '/opt/ml/input/data/train', # path where sagemaker will save training dataset\n",
    "  'test_dir': '/opt/ml/input/data/test',      # path where sagemaker will save test dataset\n",
    "  'num_train_epochs': 1,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                    # batch size for training\n",
    "  'per_device_eval_batch_size': 2,                     # batch size for evaluation\n",
    "  'learning_rate': 1e-4,   \n",
    "  'gradient_accumulation_steps': 4,\n",
    "  #'model_max_length': 512                          # learning rate used during training\n",
    "  'model_max_length': 1536                          # learning rate used during training\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Model Parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8,\n",
    "}\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        #\"microbatches\": 4,\n",
    "        #\"placement_strategy\": \"spread\",\n",
    "        #\"sharded_data_parallel_degree\": 16,\n",
    "        #\"ddp_dist_backend\": \"nccl\",\n",
    "        \"pipeline_parallel_degree\": 16,\n",
    "        \"placement_strategy\": \"cluster\",\n",
    "        #\"pipeline\": \"interleaved\",\n",
    "        \"tensor_parallel_degree\": 1,\n",
    "        #\"optimize\": \"speed\",\n",
    "        \"partitions\": 16,\n",
    "        \"fp16\": True,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p4d.24xlarge'\n",
    "instance_count = 2\n",
    "#volume_size = 200\n",
    "\n",
    "# metric definition to extract the results\n",
    "'''\n",
    "metric_definitions=[\n",
    "     {'Name': 'train_runtime', 'Regex':\"train_runtime.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'train_samples_per_second', 'Regex': \"train_samples_per_second.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'epoch', 'Regex': \"epoch.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator\n",
    "environment = {'CUDA_LAUNCH_BLOCKING': '1'}\n",
    "huggingface_estimator = HuggingFace(entry_point='train-llama-no-specail-token.py',\n",
    "                                    source_dir           = '.', \n",
    "                                    #metrics_definition=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    #volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    #transformers_version='4.26.0',\n",
    "                                    #pytorch_version='1.13.1',\n",
    "                                    #py_version='py39',\n",
    "                                    transformers_version='4.17',\n",
    "                                    pytorch_version='1.10',\n",
    "                                    py_version='py38',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    environment = environment,\n",
    "                                    debugger_hook_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': '\"decapoda-research/llama-7b-hf\"',\n",
       " 'training_dir': '\"/opt/ml/input/data/train\"',\n",
       " 'test_dir': '\"/opt/ml/input/data/test\"',\n",
       " 'num_train_epochs': '1',\n",
       " 'per_device_train_batch_size': '2',\n",
       " 'per_device_eval_batch_size': '2',\n",
       " 'learning_rate': '0.0001',\n",
       " 'gradient_accumulation_steps': '4',\n",
       " 'model_max_length': '1536',\n",
       " 'sagemaker_mpi_enabled': 'true',\n",
       " 'sagemaker_mpi_num_of_processes_per_host': '8',\n",
       " 'sagemaker_mpi_custom_mpi_options': '\"\"',\n",
       " 'mp_parameters': '{\"pipeline_parallel_degree\": 16, \"placement_strategy\": \"cluster\", \"tensor_parallel_degree\": 1, \"partitions\": 16, \"fp16\": true, \"ddp\": true}',\n",
       " 'sagemaker_distributed_dataparallel_enabled': 'false',\n",
       " 'sagemaker_instance_type': '\"ml.p4d.24xlarge\"'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-04-06-12-46-09-325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-06 12:46:43 Starting - Starting the training job...\n",
      "2023-04-06 12:47:19 Starting - Preparing the instances for training............\n",
      "2023-04-06 12:49:18 Downloading - Downloading input data\n",
      "2023-04-06 12:49:18 Training - Downloading the training image..................\n",
      "2023-04-06 12:52:19 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:18,756 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:18,821 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:18,823 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:21,016 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git (to revision 97a3d16a6941294d7d76d24f36f26617d224278e) to /tmp/pip-install-ye2gujlb/transformers_979c76e083014cc7a07a90336c91e6e3\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-ye2gujlb/transformers_979c76e083014cc7a07a90336c91e6e3\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:19,799 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:19,865 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:19,867 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:21,878 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git (to revision 97a3d16a6941294d7d76d24f36f26617d224278e) to /tmp/pip-install-jarfgee_/transformers_627aa7c172cb46a89de34e337c79ed11\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-jarfgee_/transformers_627aa7c172cb46a89de34e337c79ed11\u001b[0m\n",
      "\u001b[35mRunning command git rev-parse -q --verify 'sha^97a3d16a6941294d7d76d24f36f26617d224278e'\u001b[0m\n",
      "\u001b[35mRunning command git fetch -q https://github.com/huggingface/transformers.git 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mRunning command git rev-parse -q --verify 'sha^97a3d16a6941294d7d76d24f36f26617d224278e'\u001b[0m\n",
      "\u001b[34mRunning command git fetch -q https://github.com/huggingface/transformers.git 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mRunning command git checkout -q 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mRunning command git checkout -q 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mResolved https://github.com/huggingface/transformers.git to commit 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.10.1\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 44.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.16.0\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.16.0-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.7/199.7 kB 41.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting datasets==2.10.1\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting accelerate==0.16.0\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.16.0-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.7/199.7 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 22.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting deepspeed==0.8.0\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.8.0.tar.gz (749 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 749.9/749.9 kB 92.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 25.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.0\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.0.tar.gz (749 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 749.9/749.9 kB 96.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting ninja\u001b[0m\n",
      "\u001b[35mDownloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 27.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting rouge-score\u001b[0m\n",
      "\u001b[35mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 MB 28.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.3.5.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (21.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.70.13)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (3.8.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.64.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2022.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 3)) (5.9.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 3)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting ninja\u001b[0m\n",
      "\u001b[34mDownloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 39.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting hjson\u001b[0m\n",
      "\u001b[35mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 14.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting py-cpuinfo\u001b[0m\n",
      "\u001b[35mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mCollecting pydantic\u001b[0m\n",
      "\u001b[35mDownloading pydantic-1.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 112.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e->-r requirements.txt (line 1)) (2022.9.13)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e->-r requirements.txt (line 1)) (0.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e->-r requirements.txt (line 1)) (3.8.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.8/199.8 kB 47.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting absl-py\u001b[0m\n",
      "\u001b[35mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting nltk\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 83.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (6.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (2.0.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (21.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.10.1->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.9.24)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 7)) (8.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 7)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.2.1)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: deepspeed, transformers, rouge-score\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 MB 29.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2022.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (3.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 3)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate==0.16.0->-r requirements.txt (line 3)) (5.9.2)\u001b[0m\n",
      "\u001b[34mCollecting hjson\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 18.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pydantic\u001b[0m\n",
      "\u001b[34mDownloading pydantic-1.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 116.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e->-r requirements.txt (line 1)) (2022.9.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e->-r requirements.txt (line 1)) (0.13.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.8/199.8 kB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e->-r requirements.txt (line 1)) (3.8.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 33.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 92.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.10.1->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 7)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 7)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, transformers, rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752134 sha256=c3d8053b96fa942b6303fe2c45af7802da2f5427b5de28c887a1d6a3a348d1a4\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/81/36/bf/0cfa96bf12ce3d1a6ee388f8e26c9d6eb51923b9b9f0b03a4f\u001b[0m\n",
      "\u001b[35mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752132 sha256=e40083eb63df8faef6d1a7d3507bbdb9b4566e860d12a59399d61e53087f9f23\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/81/36/bf/0cfa96bf12ce3d1a6ee388f8e26c9d6eb51923b9b9f0b03a4f\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6756543 sha256=310c55e75841cc4e68b174f0b8eda734996f3754e2c60367d03406235b23a716\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5c/1d/64/c9c142433b568c604e359bd903fe1424c3707274fecc075ace\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=9dc2018b18798876595175542ebc6336bb687f25fd9801dcd214090ffb7eeab3\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed transformers rouge-score\u001b[0m\n",
      "\u001b[35mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6756543 sha256=bc75a2e4aa5ee3fd6b041a72bd6bb11db96d582f1b3379d71630b35a2d187b85\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/5c/1d/64/c9c142433b568c604e359bd903fe1424c3707274fecc075ace\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=b9cd5dae54612df3d4e21d1060c88ffd834dafba49eedacd2d67095264c93e2c\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\u001b[0m\n",
      "\u001b[35mSuccessfully built deepspeed transformers rouge-score\u001b[0m\n",
      "\u001b[35mInstalling collected packages: py-cpuinfo, ninja, hjson, bitsandbytes, pydantic, nltk, absl-py, rouge-score, huggingface-hub, deepspeed, accelerate, transformers, datasets, evaluate\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, ninja, hjson, bitsandbytes, pydantic, nltk, absl-py, rouge-score, huggingface-hub, deepspeed, accelerate, transformers, datasets, evaluate\u001b[0m\n",
      "\u001b[35mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[35mFound existing installation: huggingface-hub 0.10.0\u001b[0m\n",
      "\u001b[35mUninstalling huggingface-hub-0.10.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled huggingface-hub-0.10.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.17.0\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.17.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.17.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.10.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.10.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.10.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.17.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.17.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.17.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[35mFound existing installation: datasets 1.18.4\u001b[0m\n",
      "\u001b[35mUninstalling datasets-1.18.4:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled datasets-1.18.4\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-1.4.0 accelerate-0.16.0 bitsandbytes-0.37.2 datasets-2.10.1 deepspeed-0.8.0 evaluate-0.4.0 hjson-3.1.0 huggingface-hub-0.13.3 ninja-1.11.1 nltk-3.8.1 py-cpuinfo-9.0.0 pydantic-1.10.7 rouge-score-0.1.2 transformers-4.28.0.dev0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.2.2 -> 23.0.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:51,336 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:51,336 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:51,474 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:51,474 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:51,476 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:51,480 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:51,480 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.147.254.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:52,487 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:52,487 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.147.254.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 1.18.4\u001b[0m\n",
      "\u001b[34mUninstalling datasets-1.18.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-1.18.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 accelerate-0.16.0 bitsandbytes-0.37.2 datasets-2.10.1 deepspeed-0.8.0 evaluate-0.4.0 hjson-3.1.0 huggingface-hub-0.13.3 ninja-1.11.1 nltk-3.8.1 py-cpuinfo-9.0.0 pydantic-1.10.7 rouge-score-0.1.2 transformers-4.28.0.dev0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:52,754 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:52,754 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:53,498 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:52,891 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:52,891 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:52,900 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:53,027 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:53,027 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:53,027 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:53,027 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:53,031 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:53,625 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:53,625 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:53,625 sagemaker-training-toolkit INFO     Worker algo-1 available for communication\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:53,625 sagemaker-training-toolkit INFO     Env Hosts: ['algo-2', 'algo-1'] Hosts: ['algo-2:8', 'algo-1:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:53,626 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[35m2023-04-06 12:53:53,696 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"model_max_length\": 1536,\n",
      "        \"model_name\": \"decapoda-research/llama-7b-hf\",\n",
      "        \"mp_parameters\": {\n",
      "            \"pipeline_parallel_degree\": 16,\n",
      "            \"placement_strategy\": \"cluster\",\n",
      "            \"tensor_parallel_degree\": 1,\n",
      "            \"partitions\": 16,\n",
      "            \"fp16\": true,\n",
      "            \"ddp\": true\n",
      "        },\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"per_device_eval_batch_size\": 2,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"test_dir\": \"/opt/ml/input/data/test\",\n",
      "        \"training_dir\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-04-06-12-46-09-325\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-514385905925/huggingface-pytorch-training-2023-04-06-12-46-09-325/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-llama-no-specail-token\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-llama-no-specail-token.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"gradient_accumulation_steps\":4,\"learning_rate\":0.0001,\"model_max_length\":1536,\"model_name\":\"decapoda-research/llama-7b-hf\",\"mp_parameters\":{\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1},\"num_train_epochs\":1,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"test_dir\":\"/opt/ml/input/data/test\",\"training_dir\":\"/opt/ml/input/data/train\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train-llama-no-specail-token.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train-llama-no-specail-token\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-514385905925/huggingface-pytorch-training-2023-04-06-12-46-09-325/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"gradient_accumulation_steps\":4,\"learning_rate\":0.0001,\"model_max_length\":1536,\"model_name\":\"decapoda-research/llama-7b-hf\",\"mp_parameters\":{\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1},\"num_train_epochs\":1,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"test_dir\":\"/opt/ml/input/data/test\",\"training_dir\":\"/opt/ml/input/data/train\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-pytorch-training-2023-04-06-12-46-09-325\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-514385905925/huggingface-pytorch-training-2023-04-06-12-46-09-325/source/sourcedir.tar.gz\",\"module_name\":\"train-llama-no-specail-token\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-llama-no-specail-token.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--gradient_accumulation_steps\",\"4\",\"--learning_rate\",\"0.0001\",\"--model_max_length\",\"1536\",\"--model_name\",\"decapoda-research/llama-7b-hf\",\"--mp_parameters\",\"ddp=True,fp16=True,partitions=16,pipeline_parallel_degree=16,placement_strategy=cluster,tensor_parallel_degree=1\",\"--num_train_epochs\",\"1\",\"--per_device_eval_batch_size\",\"2\",\"--per_device_train_batch_size\",\"2\",\"--test_dir\",\"/opt/ml/input/data/test\",\"--training_dir\",\"/opt/ml/input/data/train\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_MAX_LENGTH=1536\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_NAME=decapoda-research/llama-7b-hf\u001b[0m\n",
      "\u001b[35mSM_HP_MP_PARAMETERS={\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1}\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[35mSM_HP_TEST_DIR=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[35mSM_HP_TRAINING_DIR=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mmpirun --host algo-2:8,algo-1:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x FI_PROVIDER=efa -x FI_EFA_USE_DEVICE_RDMA=1 -x NCCL_PROTO=simple -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_GRADIENT_ACCUMULATION_STEPS -x SM_HP_LEARNING_RATE -x SM_HP_MODEL_MAX_LENGTH -x SM_HP_MODEL_NAME -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_TEST_DIR -x SM_HP_TRAINING_DIR -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train-llama-no-specail-token.py --gradient_accumulation_steps 4 --learning_rate 0.0001 --model_max_length 1536 --model_name decapoda-research/llama-7b-hf --mp_parameters ddp=True,fp16=True,partitions=16,pipeline_parallel_degree=16,placement_strategy=cluster,tensor_parallel_degree=1 --num_train_epochs 1 --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --test_dir /opt/ml/input/data/test --training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35m[2023-04-06 12:53:55.269: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-1,10.0.147.254' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35mData for JOB [22689,1] offset 0 Total slots allocated 16\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [22689,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:56,038 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=325, name='orted', status='sleeping', started='12:53:55')]\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:56,038 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=325, name='orted', status='sleeping', started='12:53:55')]\u001b[0m\n",
      "\u001b[34m2023-04-06 12:53:56,038 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=325, name='orted', status='sleeping', started='12:53:55')]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-04-06 12:53:57.344: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-04-06 12:53:57.378: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-04-06 12:53:57.378: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-04-06 12:53:57.384: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-04-06 12:53:57.386: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-04-06 12:53:57.389: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-04-06 12:53:57.411: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-04-06 12:53:57.411: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:53:57.413: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-04-06 12:53:57.418: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-04-06 12:53:57.418: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-04-06 12:53:57.438: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-04-06 12:53:57.438: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-04-06 12:53:57.438: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-04-06 12:53:57.458: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-04-06 12:53:57.459: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-04-06 12:53:59.922: I smdistributed/modelparallel/torch/state_mod.py:162] [4] Finished initializing torch distributed process groups. pp_rank: 4, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-04-06 12:53:59.922: I smdistributed/modelparallel/torch/state_mod.py:162] [12] Finished initializing torch distributed process groups. pp_rank: 12, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-04-06 12:53:59.924: I smdistributed/modelparallel/torch/state_mod.py:162] [6] Finished initializing torch distributed process groups. pp_rank: 6, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-04-06 12:53:59.925: I smdistributed/modelparallel/torch/state_mod.py:162] [5] Finished initializing torch distributed process groups. pp_rank: 5, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-04-06 12:53:59.925: I smdistributed/modelparallel/torch/state_mod.py:162] [7] Finished initializing torch distributed process groups. pp_rank: 7, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-04-06 12:53:59.926: I smdistributed/modelparallel/torch/state_mod.py:162] [15] Finished initializing torch distributed process groups. pp_rank: 15, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-04-06 12:53:59.926: I smdistributed/modelparallel/torch/state_mod.py:162] [9] Finished initializing torch distributed process groups. pp_rank: 9, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-04-06 12:53:59.928: I smdistributed/modelparallel/torch/state_mod.py:162] [3] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-04-06 12:53:59.928: I smdistributed/modelparallel/torch/state_mod.py:162] [10] Finished initializing torch distributed process groups. pp_rank: 10, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-04-06 12:53:59.930: I smdistributed/modelparallel/torch/state_mod.py:162] [13] Finished initializing torch distributed process groups. pp_rank: 13, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:53:59.931: I smdistributed/modelparallel/torch/state_mod.py:162] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-04-06 12:53:59.931: I smdistributed/modelparallel/torch/state_mod.py:162] [8] Finished initializing torch distributed process groups. pp_rank: 8, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:53:59.931: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 8.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-04-06 12:53:59.931: I smdistributed/modelparallel/torch/state_mod.py:162] [1] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-04-06 12:53:59.931: I smdistributed/modelparallel/torch/state_mod.py:162] [14] Finished initializing torch distributed process groups. pp_rank: 14, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-04-06 12:53:59.931: I smdistributed/modelparallel/torch/state_mod.py:162] [2] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-04-06 12:53:59.932: I smdistributed/modelparallel/torch/state_mod.py:162] [11] Finished initializing torch distributed process groups. pp_rank: 11, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-04-06 12:53:59.965: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-04-06 12:53:59.976: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-04-06 12:53:59.988: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-04-06 12:53:59.993: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-04-06 12:53:59.993: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-04-06 12:53:59.994: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-04-06 12:53:59.996: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-04-06 12:53:59.996: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-04-06 12:53:59.998: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-04-06 12:54:00.000: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-04-06 12:54:00.005: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-04-06 12:54:00.009: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-04-06 12:54:00.010: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-04-06 12:54:00.010: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)lve/main/config.json:   0%|          | 0.00/427 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)lve/main/config.json: 100%|██████████| 427/427 [00:00<00:00, 122kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)lve/main/config.json:   0%|          | 0.00/427 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)lve/main/config.json: 100%|██████████| 427/427 [00:00<00:00, 61.0kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)model.bin.index.json:   0%|          | 0.00/25.5k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)model.bin.index.json: 100%|██████████| 25.5k/25.5k [00:00<00:00, 7.51MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)model.bin.index.json:   0%|          | 0.00/25.5k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)model.bin.index.json: 100%|██████████| 25.5k/25.5k [00:00<00:00, 10.6MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:01, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:00, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:00, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 341MB/s] [1,mpirank:3,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 295MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 345MB/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  65%|██████▍   | 262M/405M [00:00<00:00, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 368MB/s][1,mpirank:3,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  73%|███████▎  | 294M/405M [00:00<00:00, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  88%|████████▊ | 357M/405M [00:00<00:00, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00001-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 364MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:37,  1.16s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:36,  1.14s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:36,  1.13s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:36,  1.13s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:36,  1.14s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:37,  1.18s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:36,  1.13s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:01, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00001-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 312MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:43,  1.35s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:42,  1.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:42,  1.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:42,  1.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:43,  1.35s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:44,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:   3%|▎         | 1/33 [00:01<00:44,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:00, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 408MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:01, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:01, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:01, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  75%|███████▌  | 304M/405M [00:00<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:01, 146MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:01, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading (…)l-00002-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 339MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:02<00:37,  1.21s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:02<00:37,  1.20s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:02<00:37,  1.21s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:01, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:02<00:37,  1.19s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:02<00:37,  1.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:02<00:37,  1.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:02<00:37,  1.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  54%|█████▍    | 220M/405M [00:01<00:00, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:05, 69.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:04, 77.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:03, 107MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:02, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:02, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:02, 127MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-04-06 12:54:04.491: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:293] Configuration parameters:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   pipeline_parallel_degree: 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   microbatches: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   pipeline: interleaved\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   horovod: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   ddp: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.554: I smdistributed/modelparallel/backend/config.py:296]   ddp_port: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   ddp_dist_backend: nccl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   contiguous: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   optimize: speed\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   default_partition: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   auto_partition: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   prescaled_batch: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   active_microbatches: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   fp16: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   bf16: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   fp16_params: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   offload_activations: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   shard_optimizer_state: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   sharded_data_parallel_degree: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   delayed_parameter_initialization: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   skip_tracing: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.555: I smdistributed/modelparallel/backend/config.py:296]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.556: W smdistributed/modelparallel/backend/config.py:301] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:54:04.556: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  31%|███       | 126M/405M [00:01<00:02, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin:  96%|█████████▌| 388M/405M [00:02<00:00, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  36%|███▋      | 147M/405M [00:01<00:02, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00002-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 182MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:03<00:59,  1.90s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:03<00:58,  1.89s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:03<00:58,  1.89s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:03<00:58,  1.90s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:03<00:58,  1.90s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:03<00:59,  1.91s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:   6%|▌         | 2/33 [00:03<00:59,  1.92s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  44%|████▍     | 178M/405M [00:01<00:01, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  52%|█████▏    | 210M/405M [00:01<00:01, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:01, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  60%|█████▉    | 241M/405M [00:01<00:00, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:00, 348MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 1668\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 364MB/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 322MB/s][1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00003-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 198MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:04<00:48,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:04<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:04<00:48,  1.62s/it][1,mpirank:1,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:04<00:48,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:04<00:48,  1.60s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:04<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:04<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:00<00:01, 16.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  70%|██████▉   | 283M/405M [00:00<00:00, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 249MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 188MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00003-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 253MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:05<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:05<00:53,  1.78s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:05<00:53,  1.78s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:05<00:53,  1.80s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:01<00:10,  2.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:05<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:05<00:53,  1.80s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:   9%|▉         | 3/33 [00:05<00:54,  1.80s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  67%|██████▋   | 273M/405M [00:01<00:00, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:01, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 247MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00004-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 286MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:05<00:45,  1.55s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:06<00:45,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:05<00:45,  1.56s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:06<00:45,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:05<00:45,  1.56s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:05<00:45,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:05<00:45,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 204MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:00, 198MB/s][1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00004-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 226MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:07<00:52,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:07<00:52,  1.80s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:07<00:52,  1.80s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:07<00:52,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 199MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:02<00:24,  1.20it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:07<00:52,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:07<00:52,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  12%|█▏        | 4/33 [00:07<00:53,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:01, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:01, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 329MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00005-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 232MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:07<00:45,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:07<00:45,  1.63s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:03<00:22,  1.23it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:07<00:46,  1.65s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:07<00:45,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:07<00:45,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:07<00:46,  1.65s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:07<00:46,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:00, 382MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  73%|███████▎  | 294M/405M [00:00<00:00, 368MB/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  34%|███▎      | 136M/405M [00:00<00:00, 418MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  83%|████████▎ | 336M/405M [00:00<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00005-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 344MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:08<00:44,  1.60s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:08<00:44,  1.59s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:08<00:44,  1.59s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:08<00:44,  1.60s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:08<00:44,  1.60s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:08<00:44,  1.60s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:04<00:26,  1.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  15%|█▌        | 5/33 [00:08<00:44,  1.60s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:02, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  80%|████████  | 325M/405M [00:00<00:00, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:01, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  91%|█████████ | 367M/405M [00:00<00:00, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00006-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 399MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:04<00:23,  1.14it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:08<00:38,  1.44s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:08<00:38,  1.44s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:08<00:39,  1.44s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:08<00:39,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:01, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:08<00:39,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:08<00:39,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:08<00:39,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:01, 251MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:01, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:01, 231MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  44%|████▍     | 178M/405M [00:00<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:01, 249MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:01, 189MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  67%|██████▋   | 273M/405M [00:01<00:00, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:01, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:00, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00006-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 276MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:09<00:42,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:09<00:42,  1.56s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:05<00:30,  1.12s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:09<00:42,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:09<00:42,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:09<00:42,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:09<00:42,  1.58s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  18%|█▊        | 6/33 [00:09<00:42,  1.58s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  65%|██████▍   | 262M/405M [00:01<00:00, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 261MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:01, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:00, 327MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00007-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 261MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:06<00:28,  1.08s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:10<00:39,  1.50s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:10<00:39,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:10<00:39,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:10<00:39,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:10<00:39,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:10<00:39,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:10<00:39,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  70%|██████▉   | 283M/405M [00:00<00:00, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:02, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  80%|████████  | 325M/405M [00:00<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:03, 110MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00007-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 326MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:11<00:38,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:06<00:30,  1.17s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:11<00:38,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:11<00:38,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:11<00:38,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:11<00:38,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:11<00:38,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  21%|██        | 7/33 [00:11<00:38,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:02, 112MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:06, 56.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  28%|██▊       | 115M/405M [00:01<00:02, 108MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:05, 69.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:05, 72.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  34%|███▎      | 136M/405M [00:01<00:02, 95.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:04, 78.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:04, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  36%|███▋      | 147M/405M [00:01<00:03, 78.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:04, 74.3MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  39%|███▉      | 157M/405M [00:01<00:03, 68.3MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:04, 78.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  21%|██        | 83.9M/405M [00:01<00:04, 73.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  41%|████▏     | 168M/405M [00:01<00:03, 60.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  23%|██▎       | 94.4M/405M [00:01<00:03, 79.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  26%|██▌       | 105M/405M [00:01<00:03, 81.3MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  44%|████▍     | 178M/405M [00:02<00:04, 55.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  28%|██▊       | 115M/405M [00:01<00:03, 75.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  47%|████▋     | 189M/405M [00:02<00:03, 56.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  34%|███▎      | 136M/405M [00:01<00:02, 101MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  39%|███▉      | 157M/405M [00:01<00:02, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  52%|█████▏    | 210M/405M [00:02<00:02, 71.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  47%|████▋     | 189M/405M [00:01<00:01, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  54%|█████▍    | 220M/405M [00:02<00:02, 73.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  54%|█████▍    | 220M/405M [00:02<00:01, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  57%|█████▋    | 231M/405M [00:02<00:02, 72.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  62%|██████▏   | 252M/405M [00:02<00:00, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  60%|█████▉    | 241M/405M [00:03<00:02, 71.3MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  70%|██████▉   | 283M/405M [00:02<00:00, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  62%|██████▏   | 252M/405M [00:03<00:02, 69.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  78%|███████▊  | 315M/405M [00:02<00:00, 234MB/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  85%|████████▌ | 346M/405M [00:02<00:00, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  65%|██████▍   | 262M/405M [00:03<00:02, 61.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  93%|█████████▎| 377M/405M [00:02<00:00, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00008-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 148MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:13<00:47,  1.90s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:13<00:47,  1.89s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:13<00:47,  1.89s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:13<00:47,  1.89s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:09<00:41,  1.67s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:13<00:47,  1.90s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  67%|██████▋   | 273M/405M [00:03<00:02, 62.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:13<00:47,  1.90s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:13<00:47,  1.90s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:00, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  75%|███████▌  | 304M/405M [00:03<00:00, 106MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:00, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  83%|████████▎ | 336M/405M [00:03<00:00, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  91%|█████████ | 367M/405M [00:03<00:00, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 408MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin:  98%|█████████▊| 398M/405M [00:04<00:00, 201MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00008-of-00033.bin: 100%|██████████| 405M/405M [00:04<00:00, 99.7MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:10<00:48,  1.92s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:14<00:58,  2.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:14<00:58,  2.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:14<00:58,  2.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:14<00:58,  2.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:14<00:58,  2.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:14<00:58,  2.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  24%|██▍       | 8/33 [00:14<00:58,  2.34s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:01, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:00, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  65%|██████▍   | 262M/405M [00:00<00:00, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  34%|███▎      | 136M/405M [00:00<00:00, 367MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  75%|███████▌  | 304M/405M [00:00<00:00, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  44%|████▍     | 178M/405M [00:00<00:00, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00009-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 334MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:40,  1.70s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:40,  1.69s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:40,  1.69s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:40,  1.69s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:40,  1.70s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:40,  1.70s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:40,  1.70s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:11<00:37,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:00, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  88%|████████▊ | 357M/405M [00:00<00:00, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 396MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading (…)l-00009-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 373MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:46,  1.95s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:46,  1.96s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:46,  1.95s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:46,  1.96s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:46,  1.96s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:46,  1.95s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:11<00:40,  1.70s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  27%|██▋       | 9/33 [00:15<00:47,  1.96s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s][1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  44%|████▍     | 178M/405M [00:00<00:00, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 253MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  65%|██████▍   | 262M/405M [00:00<00:00, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 253MB/s][1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 238MB/s] [1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  91%|█████████ | 367M/405M [00:00<00:00, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00010-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 391MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:16<00:34,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:16<00:34,  1.50s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:16<00:34,  1.50s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:16<00:34,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:16<00:34,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:16<00:34,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:16<00:34,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:12<00:32,  1.41s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 302MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00010-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 282MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:17<00:41,  1.80s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:17<00:41,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:17<00:41,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:17<00:41,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:17<00:41,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:12<00:37,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:17<00:41,  1.81s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  30%|███       | 10/33 [00:17<00:41,  1.82s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:05, 72.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:01, 186MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 299MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 360MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00011-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 303MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:17<00:32,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:17<00:32,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:17<00:32,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:17<00:32,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:17<00:32,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:17<00:32,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:17<00:32,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:13<00:30,  1.40s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:04, 96.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 385MB/s][1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:02, 136MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  70%|██████▉   | 283M/405M [00:00<00:00, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:02, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  83%|████████▎ | 336M/405M [00:00<00:00, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:01, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:01, 237MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00011-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 356MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:18<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:18<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:18<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:18<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:14<00:33,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:18<00:35,  1.63s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:18<00:35,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  33%|███▎      | 11/33 [00:18<00:35,  1.63s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:01, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  44%|████▍     | 178M/405M [00:00<00:00, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:00, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 367MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  60%|█████▉    | 241M/405M [00:01<00:00, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 377MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  67%|██████▋   | 273M/405M [00:01<00:00, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  73%|███████▎  | 294M/405M [00:00<00:00, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  83%|████████▎ | 336M/405M [00:00<00:00, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  93%|█████████▎| 377M/405M [00:00<00:00, 398MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00012-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 379MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:30,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:30,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:15<00:29,  1.40s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:30,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:31,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:30,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:31,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:31,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:04, 97.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00012-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 214MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:33,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:33,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:33,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:33,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:33,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:33,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:19<00:33,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  36%|███▋      | 12/33 [00:15<00:32,  1.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:01, 265MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:00, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:00, 337MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  34%|███▎      | 136M/405M [00:00<00:01, 252MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:00, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  65%|██████▍   | 262M/405M [00:01<00:00, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00013-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 356MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:20<00:29,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:20<00:29,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:20<00:29,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:20<00:29,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:20<00:29,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:20<00:29,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:20<00:29,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:16<00:28,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 183MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 225MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00013-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 206MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:21<00:32,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:21<00:32,  1.63s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:21<00:32,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:21<00:32,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:21<00:32,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:21<00:32,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:21<00:32,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  39%|███▉      | 13/33 [00:17<00:31,  1.59s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:01, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:00, 372MB/s][1,mpirank:1,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:00, 387MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 344MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00014-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 258MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.52s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:18<00:28,  1.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin:  91%|█████████ | 367M/405M [00:00<00:00, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00014-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 375MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:02, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:22<00:28,  1.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  42%|████▏     | 14/33 [00:18<00:27,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 264MB/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:00, 313MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 207MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:01, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  44%|████▍     | 178M/405M [00:00<00:01, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  49%|████▉     | 199M/405M [00:01<00:01, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00015-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 329MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:23<00:26,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:23<00:25,  1.44s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:23<00:25,  1.44s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:19<00:25,  1.43s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:23<00:25,  1.44s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:23<00:26,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:23<00:26,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:23<00:26,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  54%|█████▍    | 220M/405M [00:01<00:01, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  60%|█████▉    | 241M/405M [00:01<00:01, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:00, 318MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  65%|██████▍   | 262M/405M [00:01<00:00, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin:  96%|█████████▌| 388M/405M [00:02<00:00, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00015-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 194MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:24<00:30,  1.67s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:24<00:29,  1.67s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:20<00:29,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:24<00:30,  1.67s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:24<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:24<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:24<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  45%|████▌     | 15/33 [00:24<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00016-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 336MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:24<00:23,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:24<00:23,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:24<00:23,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:24<00:23,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:00, 381MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:24<00:23,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:24<00:23,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:24<00:23,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:20<00:23,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 348MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:04, 84.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:02, 132MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 257MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  80%|████████  | 325M/405M [00:00<00:00, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin:  93%|█████████▎| 377M/405M [00:00<00:00, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00016-of-00033.bin: 100%|██████████| 405M/405M [00:00<00:00, 407MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:25<00:25,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:25<00:25,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:25<00:25,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:25<00:25,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:25<00:25,  1.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:25<00:25,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:25<00:25,  1.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  48%|████▊     | 16/33 [00:21<00:25,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  65%|██████▍   | 262M/405M [00:00<00:00, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:04, 89.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  75%|███████▌  | 304M/405M [00:00<00:00, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:02, 136MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:01, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 216MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00017-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 300MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:26<00:22,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:26<00:22,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:22<00:22,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:26<00:22,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:26<00:22,  1.40s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:26<00:22,  1.40s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:26<00:22,  1.40s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:26<00:22,  1.40s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 274MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:02, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 250MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 297MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)l-00017-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 294MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:27<00:23,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:22<00:23,  1.44s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:27<00:23,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:27<00:23,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:27<00:23,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:27<00:23,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:27<00:23,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  52%|█████▏    | 17/33 [00:27<00:23,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:07, 52.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 336MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:05, 64.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:05, 73.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00018-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 307MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:27<00:20,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:27<00:20,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:27<00:20,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:27<00:20,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:27<00:20,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:23<00:20,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:27<00:20,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:27<00:20,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:04, 81.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:04, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:00, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:03, 96.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:00, 368MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  23%|██▎       | 94.4M/405M [00:01<00:02, 108MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  34%|███▎      | 136M/405M [00:01<00:01, 173MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  44%|████▍     | 178M/405M [00:01<00:01, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  54%|█████▍    | 220M/405M [00:01<00:00, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  70%|██████▉   | 283M/405M [00:00<00:00, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  65%|██████▍   | 262M/405M [00:01<00:00, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00018-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 212MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:24<00:23,  1.60s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:29<00:24,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:29<00:24,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:29<00:24,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:29<00:24,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:29<00:24,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:29<00:24,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  55%|█████▍    | 18/33 [00:29<00:24,  1.62s/it][1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 232MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00019-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 282MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:29<00:19,  1.41s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:29<00:19,  1.41s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:29<00:19,  1.41s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:29<00:19,  1.41s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:29<00:19,  1.42s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:24<00:19,  1.41s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:29<00:19,  1.42s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:29<00:19,  1.42s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:00, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:00, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  31%|███       | 126M/405M [00:00<00:00, 396MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:00, 329MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  73%|███████▎  | 294M/405M [00:00<00:00, 326MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  83%|████████▎ | 336M/405M [00:00<00:00, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00019-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 358MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:26<00:20,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:30<00:20,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:30<00:20,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:30<00:20,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:30<00:20,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:30<00:20,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:30<00:20,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  58%|█████▊    | 19/33 [00:30<00:20,  1.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:03, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00020-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 267MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:30<00:18,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:03, 98.6MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:30<00:18,  1.45s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:30<00:19,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:30<00:19,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:30<00:19,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:30<00:19,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:30<00:19,  1.47s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:26<00:19,  1.46s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:02, 145MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 179MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 177MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  36%|███▋      | 147M/405M [00:01<00:02, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:02, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:02, 109MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  41%|████▏     | 168M/405M [00:01<00:02, 99.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  47%|████▋     | 189M/405M [00:01<00:02, 88.2MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  34%|███▎      | 136M/405M [00:01<00:03, 80.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  52%|█████▏    | 210M/405M [00:01<00:02, 86.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  36%|███▋      | 147M/405M [00:01<00:03, 74.2MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  57%|█████▋    | 231M/405M [00:02<00:01, 98.8MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  39%|███▉      | 157M/405M [00:01<00:03, 75.5MB/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  62%|██████▏   | 252M/405M [00:02<00:01, 113MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  41%|████▏     | 168M/405M [00:01<00:03, 78.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  44%|████▍     | 178M/405M [00:01<00:02, 75.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  67%|██████▋   | 273M/405M [00:02<00:01, 113MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  47%|████▋     | 189M/405M [00:02<00:02, 72.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  73%|███████▎  | 294M/405M [00:02<00:00, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  49%|████▉     | 199M/405M [00:02<00:02, 72.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  52%|█████▏    | 210M/405M [00:02<00:02, 74.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  78%|███████▊  | 315M/405M [00:02<00:00, 99.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  54%|█████▍    | 220M/405M [00:02<00:02, 77.6MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  57%|█████▋    | 231M/405M [00:02<00:02, 80.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  83%|████████▎ | 336M/405M [00:03<00:00, 92.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  60%|█████▉    | 241M/405M [00:02<00:01, 82.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  85%|████████▌ | 346M/405M [00:03<00:00, 93.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  62%|██████▏   | 252M/405M [00:02<00:01, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  88%|████████▊ | 357M/405M [00:03<00:00, 93.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  65%|██████▍   | 262M/405M [00:02<00:01, 87.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  91%|█████████ | 367M/405M [00:03<00:00, 90.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  67%|██████▋   | 273M/405M [00:03<00:01, 81.1MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  93%|█████████▎| 377M/405M [00:03<00:00, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  70%|██████▉   | 283M/405M [00:03<00:01, 71.3MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin:  96%|█████████▌| 388M/405M [00:03<00:00, 81.3MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  73%|███████▎  | 294M/405M [00:03<00:01, 77.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin: 100%|██████████| 405M/405M [00:03<00:00, 93.6MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00020-of-00033.bin: 100%|██████████| 405M/405M [00:03<00:00, 104MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:29<00:28,  2.21s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:34<00:28,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:34<00:28,  2.21s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:34<00:28,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:34<00:28,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:34<00:28,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:34<00:28,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  61%|██████    | 20/33 [00:34<00:29,  2.24s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  78%|███████▊  | 315M/405M [00:03<00:00, 102MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 291MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  83%|████████▎ | 336M/405M [00:03<00:00, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  91%|█████████ | 367M/405M [00:03<00:00, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  96%|█████████▌| 388M/405M [00:03<00:00, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 261MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin: 100%|██████████| 405M/405M [00:04<00:00, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00021-of-00033.bin: 100%|██████████| 405M/405M [00:04<00:00, 99.7MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:34<00:27,  2.26s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:34<00:26,  2.25s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:34<00:26,  2.25s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:34<00:27,  2.25s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:34<00:27,  2.25s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:30<00:27,  2.25s/it][1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:34<00:27,  2.25s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:34<00:27,  2.26s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:03, 101MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:03, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:03, 100MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  60%|█████▉    | 241M/405M [00:01<00:01, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:03, 98.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:03, 100MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  65%|██████▍   | 262M/405M [00:01<00:01, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:02, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:01, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  28%|██▊       | 115M/405M [00:01<00:02, 106MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 101MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  34%|███▎      | 136M/405M [00:01<00:02, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  41%|████▏     | 168M/405M [00:01<00:01, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  80%|████████  | 325M/405M [00:02<00:00, 96.2MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  49%|████▉     | 199M/405M [00:01<00:01, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  54%|█████▍    | 220M/405M [00:01<00:01, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  83%|████████▎ | 336M/405M [00:02<00:00, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  60%|█████▉    | 241M/405M [00:01<00:01, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  85%|████████▌ | 346M/405M [00:02<00:00, 85.2MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  65%|██████▍   | 262M/405M [00:02<00:01, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  91%|█████████ | 367M/405M [00:02<00:00, 95.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  73%|███████▎  | 294M/405M [00:02<00:00, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  96%|█████████▌| 388M/405M [00:02<00:00, 100MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  80%|████████  | 325M/405M [00:02<00:00, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin:  98%|█████████▊| 398M/405M [00:02<00:00, 96.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  88%|████████▊ | 357M/405M [00:02<00:00, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00021-of-00033.bin: 100%|██████████| 405M/405M [00:03<00:00, 130MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:33<00:29,  2.50s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:37<00:30,  2.51s/it][1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  96%|█████████▌| 388M/405M [00:02<00:00, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:37<00:30,  2.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:37<00:30,  2.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:37<00:30,  2.50s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:37<00:30,  2.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:37<00:30,  2.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  64%|██████▎   | 21/33 [00:37<00:30,  2.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00022-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 155MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:37<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:37<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:37<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:37<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:37<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:33<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:37<00:26,  2.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:37<00:26,  2.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:01, 327MB/s][1,mpirank:11,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:01, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:00, 307MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:02, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  44%|████▍     | 178M/405M [00:00<00:00, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:03, 111MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:03, 101MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 256MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  26%|██▌       | 105M/405M [00:01<00:03, 95.6MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  31%|███       | 126M/405M [00:01<00:02, 98.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading (…)l-00023-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 296MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:38<00:20,  2.09s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:38<00:20,  2.09s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:38<00:20,  2.08s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:34<00:20,  2.09s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:39<00:20,  2.09s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:38<00:20,  2.09s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:38<00:20,  2.10s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  36%|███▋      | 147M/405M [00:01<00:02, 115MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:38<00:21,  2.10s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:02, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  41%|████▏     | 168M/405M [00:01<00:02, 115MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  47%|████▋     | 189M/405M [00:01<00:01, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:00, 317MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  52%|█████▏    | 210M/405M [00:01<00:01, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  57%|█████▋    | 231M/405M [00:02<00:01, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  65%|██████▍   | 262M/405M [00:02<00:00, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  70%|██████▉   | 283M/405M [00:02<00:00, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  75%|███████▌  | 304M/405M [00:02<00:00, 110MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00024-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 306MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:16,  1.87s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:16,  1.87s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:16,  1.88s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:16,  1.88s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:36<00:16,  1.88s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:16,  1.88s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:16,  1.88s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:16,  1.88s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  80%|████████  | 325M/405M [00:02<00:00, 110MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:05, 71.9MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  85%|████████▌ | 346M/405M [00:02<00:00, 118MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:05, 74.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:04, 83.9MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  91%|█████████ | 367M/405M [00:03<00:00, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin:  96%|█████████▌| 388M/405M [00:03<00:00, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:05, 72.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin: 100%|██████████| 405M/405M [00:03<00:00, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00022-of-00033.bin: 100%|██████████| 405M/405M [00:03<00:00, 119MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:36<00:31,  2.85s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:41<00:31,  2.85s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:41<00:31,  2.85s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:41<00:31,  2.85s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:41<00:31,  2.85s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:04, 76.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:41<00:31,  2.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:41<00:31,  2.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  67%|██████▋   | 22/33 [00:41<00:31,  2.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:03, 101MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:00, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:02, 116MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:00, 326MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  28%|██▊       | 115M/405M [00:01<00:02, 125MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  34%|███▎      | 136M/405M [00:01<00:02, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  39%|███▉      | 157M/405M [00:01<00:01, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  44%|████▍     | 178M/405M [00:01<00:01, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  70%|██████▉   | 283M/405M [00:00<00:00, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  49%|████▉     | 199M/405M [00:01<00:01, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  54%|█████▍    | 220M/405M [00:01<00:01, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  85%|████████▌ | 346M/405M [00:01<00:00, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  78%|███████▊  | 315M/405M [00:02<00:00, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  88%|████████▊ | 357M/405M [00:02<00:00, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00023-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 257MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:42<00:24,  2.48s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:42<00:24,  2.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:38<00:24,  2.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:42<00:24,  2.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:42<00:24,  2.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  96%|█████████▌| 388M/405M [00:02<00:00, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:42<00:24,  2.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:42<00:24,  2.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  70%|██████▉   | 23/33 [00:42<00:24,  2.49s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00025-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 170MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:42<00:16,  2.04s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:42<00:16,  2.05s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:42<00:16,  2.05s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:38<00:16,  2.05s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:42<00:16,  2.05s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:42<00:16,  2.06s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:42<00:16,  2.06s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:42<00:16,  2.06s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:04, 80.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:00, 397MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:02, 134MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:00, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 189MB/s][1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 441MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 241MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  34%|███▎      | 136M/405M [00:00<00:01, 257MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  44%|████▍     | 178M/405M [00:00<00:00, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  75%|███████▌  | 304M/405M [00:00<00:00, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  88%|████████▊ | 357M/405M [00:00<00:00, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin: 100%|██████████| 405M/405M [00:00<00:00, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading (…)l-00026-of-00033.bin: 100%|██████████| 405M/405M [00:00<00:00, 436MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:12,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:12,  1.73s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:12,  1.73s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:39<00:12,  1.73s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:12,  1.73s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:12,  1.73s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:12,  1.74s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:12,  1.74s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:05, 76.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 296MB/s][1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:03, 107MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:03, 112MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00024-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 259MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:44<00:20,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:44<00:19,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:44<00:19,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:44<00:19,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:44<00:20,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:40<00:20,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:44<00:20,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  73%|███████▎  | 24/33 [00:44<00:20,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:02, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:00, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:02, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:00, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:01, 161MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 429MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:01, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 412MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  44%|████▍     | 178M/405M [00:01<00:01, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  52%|█████▏    | 210M/405M [00:01<00:00, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  75%|███████▌  | 304M/405M [00:00<00:00, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:00, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin:  88%|████████▊ | 357M/405M [00:00<00:00, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin: 100%|██████████| 405M/405M [00:00<00:00, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00025-of-00033.bin: 100%|██████████| 405M/405M [00:00<00:00, 425MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:45<00:14,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:45<00:14,  1.85s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:45<00:14,  1.85s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:45<00:14,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:41<00:14,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:45<00:14,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:45<00:14,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  76%|███████▌  | 25/33 [00:45<00:14,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  67%|██████▋   | 273M/405M [00:01<00:00, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:07, 52.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:05, 64.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:05, 69.0MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  88%|████████▊ | 357M/405M [00:02<00:00, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  93%|█████████▎| 377M/405M [00:02<00:00, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:04, 81.6MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  98%|█████████▊| 398M/405M [00:02<00:00, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00027-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 172MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:11,  1.93s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:11,  1.93s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:11,  1.93s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:11,  1.93s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:04, 82.7MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:11,  1.94s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:11,  1.94s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:41<00:11,  1.94s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:11,  1.94s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:05, 69.8MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  21%|██        | 83.9M/405M [00:01<00:03, 93.3MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:02, 130MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  26%|██▌       | 105M/405M [00:01<00:02, 117MB/s]  #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:02, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  34%|███▎      | 136M/405M [00:01<00:01, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:02, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  39%|███▉      | 157M/405M [00:01<00:01, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 176MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  44%|████▍     | 178M/405M [00:01<00:01, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  49%|████▉     | 199M/405M [00:01<00:01, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:01, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:00, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  44%|████▍     | 178M/405M [00:01<00:01, 179MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  65%|██████▍   | 262M/405M [00:01<00:00, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  49%|████▉     | 199M/405M [00:01<00:01, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  70%|██████▉   | 283M/405M [00:02<00:00, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:00, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  75%|███████▌  | 304M/405M [00:02<00:00, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  80%|████████  | 325M/405M [00:02<00:00, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  85%|████████▌ | 346M/405M [00:02<00:00, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  91%|█████████ | 367M/405M [00:02<00:00, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin:  96%|█████████▌| 388M/405M [00:02<00:00, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00026-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 143MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:48<00:15,  2.17s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:48<00:15,  2.17s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:48<00:15,  2.17s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  98%|█████████▊| 398M/405M [00:02<00:00, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:48<00:15,  2.18s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:48<00:15,  2.18s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:48<00:15,  2.18s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)l-00028-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 200MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:43<00:15,  2.18s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:48<00:09,  1.97s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  79%|███████▉  | 26/33 [00:48<00:15,  2.18s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:48<00:09,  1.97s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:48<00:09,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:48<00:09,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:48<00:09,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:48<00:09,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:48<00:09,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:44<00:09,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 274MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:07, 53.4MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:03, 106MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:00, 309MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:02, 134MB/s][1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:01, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:01, 191MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 306MB/s][1,mpirank:13,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:01, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  67%|██████▋   | 273M/405M [00:00<00:00, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  41%|████▏     | 168M/405M [00:01<00:01, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  47%|████▋     | 189M/405M [00:01<00:01, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  52%|█████▏    | 210M/405M [00:01<00:01, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00029-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 314MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:49<00:07,  1.78s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:49<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:49<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:49<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:45<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:49<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:49<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:49<00:07,  1.80s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:01, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:01, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:01, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  67%|██████▋   | 273M/405M [00:01<00:00, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:00, 329MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  49%|████▉     | 199M/405M [00:00<00:00, 336MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  80%|████████  | 325M/405M [00:02<00:00, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  91%|█████████ | 367M/405M [00:02<00:00, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin:  98%|█████████▊| 398M/405M [00:02<00:00, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  70%|██████▉   | 283M/405M [00:00<00:00, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00027-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 175MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:50<00:13,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:50<00:13,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:50<00:13,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:50<00:13,  2.22s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:46<00:13,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:50<00:13,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:50<00:13,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  82%|████████▏ | 27/33 [00:50<00:13,  2.23s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  80%|████████  | 325M/405M [00:00<00:00, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:02, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:02, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:02, 160MB/s][1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 249MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading (…)l-00030-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 296MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.69s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:46<00:05,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.68s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.69s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.69s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.70s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:   3%|▎         | 10.5M/405M [00:00<00:04, 81.5MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 231MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  41%|████▏     | 168M/405M [00:00<00:00, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:03, 113MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:02, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  62%|██████▏   | 252M/405M [00:00<00:00, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:02, 118MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:02, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin:  93%|█████████▎| 377M/405M [00:01<00:00, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00028-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 282MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:52<00:10,  2.01s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:52<00:10,  2.00s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:52<00:10,  2.00s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:47<00:10,  2.00s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:02, 124MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:52<00:10,  2.00s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:52<00:10,  2.00s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:52<00:10,  2.01s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  85%|████████▍ | 28/33 [00:52<00:10,  2.01s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  36%|███▋      | 147M/405M [00:01<00:01, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  47%|████▋     | 189M/405M [00:01<00:01, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:01, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  57%|█████▋    | 231M/405M [00:01<00:00, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:02, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  67%|██████▋   | 273M/405M [00:01<00:00, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  78%|███████▊  | 315M/405M [00:01<00:00, 281MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:02, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  28%|██▊       | 115M/405M [00:00<00:01, 148MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 293MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  34%|███▎      | 136M/405M [00:00<00:01, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00031-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 212MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:53<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:53<00:03,  1.76s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:53<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:53<00:03,  1.77s/it][1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:53<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:53<00:03,  1.78s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:48<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:53<00:03,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:01, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:   8%|▊         | 31.5M/405M [00:00<00:01, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  47%|████▋     | 189M/405M [00:01<00:01, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  16%|█▌        | 62.9M/405M [00:00<00:01, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  54%|█████▍    | 220M/405M [00:01<00:00, 200MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  23%|██▎       | 94.4M/405M [00:00<00:01, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  60%|█████▉    | 241M/405M [00:01<00:00, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  31%|███       | 126M/405M [00:00<00:01, 233MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  65%|██████▍   | 262M/405M [00:01<00:00, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:01, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  70%|██████▉   | 283M/405M [00:01<00:00, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  75%|███████▌  | 304M/405M [00:01<00:00, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  54%|█████▍    | 220M/405M [00:00<00:00, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  62%|██████▏   | 252M/405M [00:01<00:00, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  85%|████████▌ | 346M/405M [00:02<00:00, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  91%|█████████ | 367M/405M [00:02<00:00, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  83%|████████▎ | 336M/405M [00:01<00:00, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin:  98%|█████████▊| 398M/405M [00:02<00:00, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00029-of-00033.bin: 100%|██████████| 405M/405M [00:02<00:00, 167MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:54<00:08,  2.14s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:54<00:08,  2.14s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:54<00:08,  2.14s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:54<00:08,  2.15s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:54<00:08,  2.15s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:54<00:08,  2.15s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  91%|█████████ | 367M/405M [00:01<00:00, 256MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:54<00:08,  2.15s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  88%|████████▊ | 29/33 [00:50<00:08,  2.15s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:01, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  98%|█████████▊| 398M/405M [00:01<00:00, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00032-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 233MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  21%|██        | 83.9M/405M [00:00<00:00, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.78s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:50<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.78s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:   0%|          | 0.00/524M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  34%|███▎      | 136M/405M [00:00<00:00, 388MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:   2%|▏         | 10.5M/524M [00:00<00:04, 104MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:   4%|▍         | 21.0M/524M [00:00<00:04, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  60%|█████▉    | 241M/405M [00:00<00:00, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  10%|▉         | 52.4M/524M [00:00<00:02, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  73%|███████▎  | 294M/405M [00:00<00:00, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  18%|█▊        | 94.4M/524M [00:00<00:01, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  24%|██▍       | 126M/524M [00:00<00:01, 258MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  85%|████████▌ | 346M/405M [00:00<00:00, 387MB/s][1,mpirank:7,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  30%|██▉       | 157M/524M [00:00<00:01, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00030-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 379MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:55<00:05,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:55<00:05,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:55<00:05,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:55<00:05,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  38%|███▊      | 199M/524M [00:00<00:01, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:51<00:05,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:55<00:05,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:55<00:05,  1.84s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  91%|█████████ | 30/33 [00:55<00:05,  1.84s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  44%|████▍     | 231M/524M [00:00<00:01, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  13%|█▎        | 52.4M/405M [00:00<00:00, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  50%|████▉     | 262M/524M [00:01<00:00, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:00, 355MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  58%|█████▊    | 304M/524M [00:01<00:00, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  39%|███▉      | 157M/405M [00:00<00:00, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  64%|██████▍   | 336M/524M [00:01<00:00, 268MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  52%|█████▏    | 210M/405M [00:00<00:00, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  72%|███████▏  | 377M/524M [00:01<00:00, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  65%|██████▍   | 262M/405M [00:00<00:00, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  78%|███████▊  | 409M/524M [00:01<00:00, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  78%|███████▊  | 315M/405M [00:00<00:00, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  86%|████████▌ | 451M/524M [00:01<00:00, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin:  91%|█████████ | 367M/405M [00:00<00:00, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)l-00031-of-00033.bin: 100%|██████████| 405M/405M [00:00<00:00, 427MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:56<00:03,  1.58s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:52<00:03,  1.58s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  94%|█████████▍| 493M/524M [00:01<00:00, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:56<00:03,  1.58s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:56<00:03,  1.58s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:56<00:03,  1.58s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:56<00:03,  1.59s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:56<00:03,  1.59s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  94%|█████████▍| 31/33 [00:56<00:03,  1.59s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)l-00033-of-00033.bin: 100%|██████████| 524M/524M [00:01<00:00, 280MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.82s/it][1,mpirank:12,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.82s/it][1,mpirank:14,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:52<00:00,  1.82s/it][1,mpirank:8,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:52<00:00,  1.59s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:   5%|▌         | 21.0M/405M [00:00<00:02, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  10%|█         | 41.9M/405M [00:00<00:02, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  18%|█▊        | 73.4M/405M [00:00<00:01, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  26%|██▌       | 105M/405M [00:00<00:01, 255MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  36%|███▋      | 147M/405M [00:00<00:00, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  47%|████▋     | 189M/405M [00:00<00:00, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  57%|█████▋    | 231M/405M [00:00<00:00, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  65%|██████▍   | 262M/405M [00:00<00:00, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  73%|███████▎  | 294M/405M [00:01<00:00, 298MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  80%|████████  | 325M/405M [00:01<00:00, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  88%|████████▊ | 357M/405M [00:01<00:00, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin:  96%|█████████▌| 388M/405M [00:01<00:00, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00032-of-00033.bin: 100%|██████████| 405M/405M [00:01<00:00, 245MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:54<00:01,  1.61s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:58<00:01,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:58<00:01,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:58<00:01,  1.63s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:58<00:01,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:58<00:01,  1.62s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:58<00:01,  1.63s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:   0%|          | 0.00/524M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards:  97%|█████████▋| 32/33 [00:58<00:01,  1.64s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:   4%|▍         | 21.0M/524M [00:00<00:03, 167MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  10%|▉         | 52.4M/524M [00:00<00:02, 218MB/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  16%|█▌        | 83.9M/524M [00:00<00:02, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  20%|█▉        | 105M/524M [00:00<00:02, 148MB/s] #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  24%|██▍       | 126M/524M [00:00<00:02, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  30%|██▉       | 157M/524M [00:00<00:02, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  34%|███▍      | 178M/524M [00:01<00:02, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  40%|███▉      | 210M/524M [00:01<00:02, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  44%|████▍     | 231M/524M [00:01<00:01, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  48%|████▊     | 252M/524M [00:01<00:01, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  52%|█████▏    | 273M/524M [00:01<00:01, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  56%|█████▌    | 294M/524M [00:01<00:01, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  64%|██████▍   | 336M/524M [00:02<00:00, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  72%|███████▏  | 377M/524M [00:02<00:00, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  78%|███████▊  | 409M/524M [00:02<00:00, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  86%|████████▌ | 451M/524M [00:02<00:00, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  92%|█████████▏| 482M/524M [00:02<00:00, 257MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin:  98%|█████████▊| 514M/524M [00:02<00:00, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading (…)l-00033-of-00033.bin: 100%|██████████| 524M/524M [00:02<00:00, 189MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.98s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.99s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.99s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.99s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.99s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading shards: 100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.12it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:15,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.06it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:15,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:15,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:15,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.06it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.10it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.06it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.91it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.09it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.08it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.07it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.89it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.06it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:15,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.07it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.14it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.07it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:15,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.09it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.16it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.07it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.11it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.10it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:14,  1.90it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.12it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.16it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.13it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.13it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.16it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.14it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.13it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:11,  2.17it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.16it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.13it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.17it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.17it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.13it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.17it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.13it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.16it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.14it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.17it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.14it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.17it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.14it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.17it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:07,  2.18it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.14it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.13it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.18it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:08,  2.09it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:06,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.06it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:05,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.20it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.16it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.20it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.89it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.20it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.99it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 19.0kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:06,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)cial_tokens_map.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 807B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)okenizer_config.json: 100%|██████████| 141/141 [00:00<00:00, 50.6kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.20it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.20it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.71it/s][1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:02,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.89it/s][1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.87it/s][1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.86it/s][1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.87it/s][1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.83it/s][1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:13<00:01,  2.18it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.18it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.01it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  2.19it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.00it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.15it/s][1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 17.4kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 64.4MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)cial_tokens_map.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 754B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)okenizer_config.json: 100%|██████████| 141/141 [00:00<00:00, 56.1kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  1.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.86it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.96it/s][1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.84it/s][1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.84it/s][1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.00it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.82it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.93it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-04-06 12:56:11.238: I smdistributed/modelparallel/torch/model.py:137] [10] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-04-06 12:56:11.260: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [10] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-04-06 12:56:11.298 algo-1:330 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-04-06 12:56:11.424 algo-1:330 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-04-06 12:56:11.700: I smdistributed/modelparallel/torch/model.py:137] [14] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-04-06 12:56:11.721: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [14] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-04-06 12:56:11.759 algo-1:334 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-04-06 12:56:11.883 algo-1:334 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-04-06 12:56:11.896: I smdistributed/modelparallel/torch/model.py:137] [9] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-04-06 12:56:11.917: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [9] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-04-06 12:56:11.954 algo-1:329 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-04-06 12:56:12.077 algo-1:329 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-04-06 12:56:12.113: I smdistributed/modelparallel/torch/model.py:137] [12] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-04-06 12:56:12.134: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [12] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-04-06 12:56:12.166: I smdistributed/modelparallel/torch/model.py:137] [13] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-04-06 12:56:12.170 algo-1:332 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-04-06 12:56:12.187: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [13] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-04-06 12:56:12.224 algo-1:333 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-04-06 12:56:12.261: I smdistributed/modelparallel/torch/model.py:137] [15] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-04-06 12:56:12.283: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [15] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-04-06 12:56:12.293 algo-1:332 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-04-06 12:56:12.322 algo-1:335 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-04-06 12:56:12.346 algo-1:333 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-04-06 12:56:12.452 algo-1:335 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-04-06 12:56:12.465: I smdistributed/modelparallel/torch/model.py:137] [11] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-04-06 12:56:12.487: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [11] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-04-06 12:56:12.527 algo-1:331 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-04-06 12:56:12.606: I smdistributed/modelparallel/torch/model.py:137] [8] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-04-06 12:56:12.629: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [8] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/200 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-04-06 12:56:12.655 algo-1:331 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-04-06 12:56:12.669 algo-1:328 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-04-06 12:56:12.799 algo-1:328 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-04-06 12:56:15.086: I smdistributed/modelparallel/torch/model.py:137] [1] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-04-06 12:56:15.107: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [1] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-04-06 12:56:15.145 algo-2:395 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-04-06 12:56:15.270 algo-2:395 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-04-06 12:56:16.408: I smdistributed/modelparallel/torch/model.py:137] [3] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-04-06 12:56:16.428: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [3] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-04-06 12:56:16.445: I smdistributed/modelparallel/torch/model.py:137] [2] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-04-06 12:56:16.464 algo-2:397 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-04-06 12:56:16.466: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [2] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-04-06 12:56:16.503 algo-2:396 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-04-06 12:56:16.587 algo-2:397 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-04-06 12:56:16.627 algo-2:396 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-04-06 12:56:16.637: I smdistributed/modelparallel/torch/model.py:137] [7] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-04-06 12:56:16.658: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [7] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-04-06 12:56:16.695 algo-2:401 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-04-06 12:56:16.714: I smdistributed/modelparallel/torch/model.py:137] [4] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-04-06 12:56:16.734: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [4] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-04-06 12:56:16.771 algo-2:398 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-04-06 12:56:16.814 algo-2:401 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-04-06 12:56:16.893 algo-2:398 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:16.956: I smdistributed/modelparallel/torch/model.py:137] [0] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:16.977: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [0] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/200 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:17.014 algo-2:394 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-04-06 12:56:17.112: I smdistributed/modelparallel/torch/model.py:137] [6] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-04-06 12:56:17.134: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [6] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:17.136 algo-2:394 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:17.142: I smdistributed/modelparallel/torch/worker.py:300] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-04-06 12:56:17.167: I smdistributed/modelparallel/torch/model.py:137] [5] FP16_Module initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-04-06 12:56:17.174 algo-2:400 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-04-06 12:56:17.188: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [5] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-04-06 12:56:17.224 algo-2:399 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-04-06 12:56:17.295 algo-2:400 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-04-06 12:56:17.345 algo-2:399 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [77,0,0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [78[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [79,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [80[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [84,0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [89,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [94,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [67[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [68,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [69,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [70,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [77[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [78,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [85,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [94[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [95,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [32,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [42,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [43,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [48,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [49[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [53,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [57,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [58[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [33,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [34\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [39,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [40[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [42,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [43,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [50,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [51[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [52,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [53[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [56,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [57,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [60,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [0,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [6,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [7[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [8[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [11,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [17,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [18[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [22,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [23,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [96[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [97,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [98,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [99,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [101[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [102,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [106,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [107[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [119[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [10,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [10,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [10,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [10,0,0], thread: [127,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:19.558: C smdistributed/modelparallel/torch/worker.py:110] [0] Hit an exception for 0/0 on thread 0: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:19.559: C smdistributed/modelparallel/torch/worker.py:115] [0]   File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 506, in _thread_compute\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    self.thread_execute_tracing(req)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 309, in thread_execute_tracing\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    self._exec_trace_on_device(req, device)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 397, in fork_torch_rng_state\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    torch.cuda.set_rng_state(orig_cuda_rng_state)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 64, in set_rng_state\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    _lazy_call(cb)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 153, in _lazy_call\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    callable()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 62, in cb\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    default_generator.set_state(new_state_copy)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:19.559: C smdistributed/modelparallel/torch/worker.py:116] [0] Parent exec stack []\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-04-06 12:56:19.559: C smdistributed/modelparallel/torch/worker.py:117] [0] Req <TraceReq::mb:0, requester:0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/sparse.py\", line 158, in forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return F.embedding(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\", line 2044, in embedding\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 575, in forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    inputs_embeds = self.embed_tokens(input_ids)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    raise e\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 486, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError: [1,mpirank:0,algo-1]<stderr>:CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 772, in forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    outputs = self.model(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    raise e\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 486, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/fp16/fp16.py\", line 202, in forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return fp16_to_fp32(self.module(*(fp32_to_fp16(inputs)), **kwargs))\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    raise e\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 486, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 440, in forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    raise e\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 486, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1329, in forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return self.module(*args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    raise e\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 486, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 392, in fork_torch_rng_state\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 309, in thread_execute_tracing\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self._exec_trace_on_device(req, device)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 268, in _exec_trace_on_device\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    outputs = step_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer_pt_utils.py\", line 1076, in smp_forward_backward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    outputs = model(**inputs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    raise e\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 486, in record_execution_time\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1177, in _step\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    yield\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 261, in __call__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    state.exec_server.run_step_leader(mb_args, mb_kwargs, self.id)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 348, in run_step_leader\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.execute_request(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 119, in execute_request\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    chosen_worker.execute(req)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 155, in execute\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self._resume_thread_common()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 186, in _resume_thread_common\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self._check_queue_after_thread_return()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 121, in _check_queue_after_thread_return\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self._check_exception()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 118, in _check_exception\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    raise self.exception\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 506, in _thread_compute\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.thread_execute_tracing(req)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 309, in thread_execute_tracing\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self._exec_trace_on_device(req, device)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 397, in fork_torch_rng_state\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    torch.cuda.set_rng_state(orig_cuda_rng_state)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 64, in set_rng_state\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    _lazy_call(cb)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 153, in _lazy_call\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    callable()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 62, in cb\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    default_generator.set_state(new_state_copy)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError[1,mpirank:0,algo-1]<stderr>:: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    main()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"train-llama-no-specail-token.py\", line 145, in <module>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    trainer.train()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1633, in train\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    return inner_training_loop(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    tr_loss_step = self.training_step(model, inputs)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 2640, in training_step\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 263, in __call__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    state.exec_server.run_step_follower()\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1213, in _step\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    hook(self, state.optimizer)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/optimizers/optimizer.py\", line 539, in <lambda>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    state.model.register_post_step_hook(lambda mod, opt: opt.init_master_params())\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/fp16/fp16.py\", line 364, in init_master_params\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    self.fp32_param_buffer = torch.empty(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:RuntimeError\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [3[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [14[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [18,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [19[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [23,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [24\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [29,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [96,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [97,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [101,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [102[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [108,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0[1,mpirank:0,algo-1]<stderr>:], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699[1,mpirank:0,algo-1]<stderr>:: indexSelectLargeIndex: block: [11,0,0], thread: [110,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [111[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [115,0[1,mpirank:0,algo-1]<stderr>:,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [116,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [117,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [118[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu[1,mpirank:0,algo-1]<stderr>::699: indexSelectLargeIndex: block: [11,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize[1,mpirank:0,algo-1]<stderr>:` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [122,0,0[1,mpirank:0,algo-1]<stderr>:] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [123[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0[1,mpirank:0,algo-1]<stderr>:,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex[1,mpirank:0,algo-1]<stderr>:: block: [11,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [126[1,mpirank:0,algo-1]<stderr>:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11[1,mpirank:0,algo-1]<stderr>:,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[35ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[35mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[22689,1],0]\n",
      "  Exit code:    1\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-04-06 12:56:24,490 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:56:24,490 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-04-06 12:56:24,492 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[35m2023-04-06 12:56:24,492 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mExitCode 1\u001b[0m\n",
      "\u001b[35mErrorMessage \"RuntimeError: CUDA error: device-side assert triggered\n",
      " \n",
      " During handling of the above exception, another exception occurred\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\n",
      " yield\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      " output = original_forward(self, *args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 575, in forward\n",
      " inputs_embeds = self.embed_tokens(input_ids)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      " return forward_call(*input, **kwargs)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n",
      " raise e\n",
      " File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      " self.gen.throw(type, value, traceback)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 486, in record_execution_time\n",
      " end.record(current_stream)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\n",
      " super(Event, self).record(stream)\n",
      " RuntimeError: :CUDA error: device-side assert triggered\n",
      " File \"/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 772, in forward\n",
      " outputs = self.model(\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/fp16/fp16.py\", line 202, in forward\n",
      " return fp16_to_fp32(self.module(*(fp32_to_fp16(inputs)), **kwargs))\n",
      " RuntimeError\n",
      " CUDA error: device-side assert triggered\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 440, in forward\n",
      " return self.module(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1329, in forward\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 392, in fork_torch_rng_state\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 309, in thread_execute_tracing\n",
      " self._exec_trace_on_device(req, device)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 268, in _exec_trace_on_device\n",
      " outputs = step_fn(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer_pt_utils.py\", line 1076, in smp_forward_backward\n",
      " outputs = model(**inputs)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1177, in _step\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 261, in __call__\n",
      " state.exec_server.run_step_leader(mb_args, mb_kwargs, self.id)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 348, in run_step_leader\n",
      " self.execute_request(\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 119, in execute_request\n",
      " chosen_worker.execute(req)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 155, in execute\n",
      " self._resume_thread_common()\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 186, in _resume_thread_common\n",
      " self._check_queue_after_thread_return()\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 121, in _check_queue_after_thread_return\n",
      " self._check_exception()\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 118, in _check_exception\n",
      " raise self.exception\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 506, in _thread_compute\n",
      " self.thread_execute_tracing(req)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 397, in fork_torch_rng_state\n",
      " torch.cuda.set_rng_state(orig_cuda_rng_state)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 64, in set_rng_state\n",
      " _lazy_call(cb)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 153, in _lazy_call\n",
      " callable()\n",
      " File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 62, in cb\n",
      " default_generator.set_state(new_state_copy)\n",
      " RuntimeError:: CUDA error: device-side assert triggered\n",
      " File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      " return _run_code(code, main_globals, None,\n",
      " File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      " exec(code, run_globals)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      " main()\n",
      " File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      " run_command_line(args)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      " run_path(sys.argv[0], run_name='__main__')\n",
      " File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      " return _run_module_code(code, init_globals, run_name,\n",
      " File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      " _run_code(code, mod_globals, init_globals,\n",
      " File \"train-llama-no-specail-token.py\", line 145, in <module>\n",
      " trainer.train()\n",
      " File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1633, in train\n",
      " return inner_training_loop(\n",
      " File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      " tr_loss_step = self.training_step(model, inputs)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 2640, in training_step\n",
      " loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 263, in __call__\n",
      " state.exec_server.run_step_follower()\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1213, in _step\n",
      " hook(self, state.optimizer)\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/optimizers/optimizer.py\", line 539, in <lambda>\n",
      " state.model.register_post_step_hook(lambda mod, opt: opt.init_master_params())\n",
      " File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/fp16/fp16.py\", line 364, in init_master_params\n",
      " self.fp32_param_buffer = torch.empty(\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [3:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu::699: indexSelectLargeIndex: block: [11,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex\n",
      " block: [11,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699\n",
      " indexSelectLargeIndex: block: [11,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [14:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0:,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex:: block: [11,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu::699: indexSelectLargeIndex: block: [11,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize:` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [18,0:,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [19:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0:,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex:: block: [11,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu::699: indexSelectLargeIndex: block: [11,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [23,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [24\n",
      " ,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0:,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0:], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11:,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [29,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0:], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [96,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [97,0:,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0:], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11:,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [101,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [102:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0:,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex:: block: [11,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu::699: indexSelectLargeIndex: block: [11,0,0:], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11:,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [108,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0:], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699:: indexSelectLargeIndex: block: [11,0,0], thread: [110,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [111:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0:,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex:: block: [11,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu::699: indexSelectLargeIndex: block: [11,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize:` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [115,0:,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [116,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [117,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [118:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0:,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex:: block: [11,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu::699: indexSelectLargeIndex: block: [11,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize:` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [122,0,0:] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [123:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0:,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex:: block: [11,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11,0,0], thread: [126:,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " /codebuild/output/src762783086/src/aten/src/ATen/native/cuda/Indexing.cu:699: indexSelectLargeIndex: block: [11:,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      " --------------------------------------------------------------------------\n",
      " Primary job  terminated normally, but 1 process returned\n",
      " a non-zero exit code. Per user-direction, the job has been aborted.\n",
      " mpirun.real detected that one or more processes exited with non-zero status, thus causing\n",
      " the job to be terminated. The first process to do so was\n",
      " Process name: [[22689,1],0]\n",
      " Exit code:    1\"\u001b[0m\n",
      "\u001b[35mCommand \"mpirun --host algo-2:8,algo-1:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x FI_PROVIDER=efa -x FI_EFA_USE_DEVICE_RDMA=1 -x NCCL_PROTO=simple -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_GRADIENT_ACCUMULATION_STEPS -x SM_HP_LEARNING_RATE -x SM_HP_MODEL_MAX_LENGTH -x SM_HP_MODEL_NAME -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_TEST_DIR -x SM_HP_TRAINING_DIR -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train-llama-no-specail-token.py --gradient_accumulation_steps 4 --learning_rate 0.0001 --model_max_length 1536 --model_name decapoda-research/llama-7b-hf --mp_parameters ddp=True,fp16=True,partitions=16,pipeline_parallel_degree=16,placement_strategy=cluster,tensor_parallel_degree=1 --num_train_epochs 1 --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --test_dir /opt/ml/input/data/test --training_dir /opt/ml/input/data/train\"\u001b[0m\n",
      "\u001b[35m2023-04-06 12:56:24,492 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\u001b[34m2023-04-06 12:56:24,510 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[34m2023-04-06 12:56:24,511 sagemaker-training-toolkit INFO     process psutil.Process(pid=325, name='orted', status='terminated', started='12:53:55') terminated with exit code None\u001b[0m\n",
      "\u001b[34m2023-04-06 12:56:24,511 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=325, name='orted', status='terminated', started='12:53:55')] alive: []\u001b[0m\n",
      "\u001b[34m2023-04-06 12:56:24,511 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\n",
      "2023-04-06 12:56:36 Uploading - Uploading generated training model\n",
      "2023-04-06 12:56:47 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2023-04-06-12-46-09-325: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"RuntimeError: CUDA error: device-side assert triggered\n \n During handling of the above exception, another exception occurred\n Traceback (most recent call last)\n File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\n yield\n File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n output = original_forward(self, *args, **kwargs)\n File \"/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 575, in forward\n inputs_embeds = self.embed_tokens(input_ids)\n File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n return forward_call(*input, **kwargs)\n File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n raise e\n File \"/opt/conda/lib/python3.8/contextlib.py\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ff85259e9638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2345\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   4667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4669\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4671\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   4177\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4178\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4179\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4180\u001b[0m             )\n\u001b[1;32m   4181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2023-04-06-12-46-09-325: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"RuntimeError: CUDA error: device-side assert triggered\n \n During handling of the above exception, another exception occurred\n Traceback (most recent call last)\n File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 484, in record_execution_time\n yield\n File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n output = original_forward(self, *args, **kwargs)\n File \"/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 575, in forward\n inputs_embeds = self.embed_tokens(input_ids)\n File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n return forward_call(*input, **kwargs)\n File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n raise e\n File \"/opt/conda/lib/python3.8/contextlib.py\""
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "#test_input_path = 's3://sagemaker-us-east-1-514385905925/samples/datasets/test001/test'\n",
    "test_input_path = 's3://sagemaker-us-east-1-514385905925/samples/datasets/lala-no-special-token-test0406/test/'\n",
    "\n",
    "\n",
    "data = {\n",
    "    'train': test_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g5.48xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

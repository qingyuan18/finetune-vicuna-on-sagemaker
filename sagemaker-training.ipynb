{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4416c96",
   "metadata": {},
   "source": [
    "# An sample to finetune vicuna on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95febd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download training script from github\n",
    "!git clone https://github.com/lm-sys/FastChat.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5a450",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2513fac",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f942fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd25c4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"pinkmanlove/llama-7b-hf\"#decapoda-research/llama-13b-hf\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a2647",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918df42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de4454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama/pretrain/pinkmanlove/llama-7b-hf/ \n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d498",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2057f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "# RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\n",
    "\n",
    "RUN pip3 uninstall -y deepspeed && pip3 install deepspeed\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ee553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b0fb6",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53800617",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-vicuna-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d98c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6dc0d",
   "metadata": {},
   "source": [
    "### Generate the deepspeed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b56758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ds.json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"auto_cast\": false,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": \"auto\",\n",
    "      \"betas\": \"auto\",\n",
    "      \"eps\": \"auto\",\n",
    "      \"weight_decay\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": \"auto\",\n",
    "      \"warmup_max_lr\": \"auto\",\n",
    "      \"warmup_num_steps\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"sub_group_size\": 1e9,\n",
    "    \"reduce_bucket_size\": \"auto\",\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5fc567",
   "metadata": {},
   "source": [
    "**Generate training entrypoint script.**\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e3d70",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/7B/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PYTHONPATH=./FastChat:$PYTHONPATH\n",
    "# echo $PYTHONPATH\n",
    "# cd FastChat && pip install -e . && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile train.sh\n",
    "# #!/bin/bash\n",
    "\n",
    "# chmod +x ./s5cmd\n",
    "# ./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "\n",
    "# cd FastChat && pip install -e . && cd ..\n",
    "\n",
    "# torchrun --nproc_per_node=8 --master_port=20001 FastChat/fastchat/train/train_mem.py \\\n",
    "#     --model_name_or_path /tmp/llama_pretrain/  \\\n",
    "#     --data_path sharegpt_test.json \\\n",
    "#     --bf16 True \\\n",
    "#     --output_dir /tmp/llama_out \\\n",
    "#     --num_train_epochs 3 \\\n",
    "#     --per_device_train_batch_size 2 \\\n",
    "#     --per_device_eval_batch_size 2 \\\n",
    "#     --gradient_accumulation_steps 16 \\\n",
    "#     --evaluation_strategy \"no\" \\\n",
    "#     --save_strategy \"steps\" \\\n",
    "#     --save_steps 1200 \\\n",
    "#     --save_total_limit 10 \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --lr_scheduler_type \"cosine\" \\\n",
    "#     --logging_steps 1 \\\n",
    "#     --fsdp \"full_shard auto_wrap\" \\\n",
    "#     --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n",
    "#     --tf32 True \\\n",
    "#     --model_max_length 2048 \\\n",
    "#     --gradient_checkpointing True \\\n",
    "#     --lazy_preprocess True \\\n",
    "#     --report_to \"none\"\n",
    "\n",
    "# if [ $? -eq 1 ]; then\n",
    "#     echo \"Training script error, please check CloudWatch logs\"\n",
    "#     exit 1\n",
    "# fi\n",
    "\n",
    "# ./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llama/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c6fa5",
   "metadata": {},
   "source": [
    "We will use deepspeed launch instead of torch run.\n",
    "\n",
    "(When use torch run, there will report tensor size not match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c51767",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "We modified some parts of ```FastChat/fastchat/train/train.py```, such as how to save model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef55e2",
   "metadata": {},
   "source": [
    "Here we use sample dataset - sharegpt_test.json for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cedcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ds-train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "\n",
    "cd FastChat && pip install -e . && cd ..\n",
    "\n",
    "\n",
    "deepspeed --num_gpus=8 FastChat/fastchat/train/train_mem.py \\\n",
    "    --deepspeed ds.json \\\n",
    "    --model_name_or_path \"/tmp/llama_pretrain/\" \\\n",
    "    --data_path sharegpt_test.json \\\n",
    "    --output_dir \"/tmp/llama_out\" \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size  2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --save_steps 2000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --cache_dir '/tmp' \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    --fp16 True \\\n",
    "    --tf32 True \\\n",
    "    --report_to \"none\"\n",
    "\n",
    "if [ $? -eq 1 ]; then\n",
    "    echo \"Training script error, please check CloudWatch logs\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llama/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517889b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad795754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "train_data_path = f's3://{sagemaker_default_bucket}/llama/train_data/'\n",
    "\n",
    "inputs = {'train': train_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab36100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'vicuna-demo'         \n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train.sh',\n",
    "                      source_dir='./',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "estimator.fit()\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e673c",
   "metadata": {},
   "source": [
    "You could find the model path in S3 from above logs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

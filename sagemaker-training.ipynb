{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4416c96",
   "metadata": {},
   "source": [
    "# An sample to finetune vicuna on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95febd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158e94ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82a0c829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'FastChat'...\n",
      "remote: Enumerating objects: 1816, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 1816 (delta 1), reused 2 (delta 0), pack-reused 1801\u001b[K\n",
      "Receiving objects: 100% (1816/1816), 29.16 MiB | 35.13 MiB/s, done.\n",
      "Resolving deltas: 100% (1174/1174), done.\n"
     ]
    }
   ],
   "source": [
    "## download training script from github\n",
    "!rm -rf ./FastChat\n",
    "!git clone https://github.com/lm-sys/FastChat.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5a450",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2513fac",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f942fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (0.11.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface_hub) (4.11.4)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->huggingface_hub) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cd25c4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffcfe41e692486ebc5044462879562e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e241f502964fa581b3867e22291179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2b64b718b14026b2dfda326ac3f00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08304e9b0ad4902879b3ac44ca44e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57abadbe15834a0284235d0f4c33493e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bd9582ec494b6e8bf0bc8b6b662443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24aafdcfe3674fe98d2f0902f1b0d9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb60ee55c4941baa79157d774c8a2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a8a49be4f04be19eb121d5881628cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"pinkmanlove/llama-7b-hf\"#decapoda-research/llama-13b-hf\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a2647",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "918df42f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/config.json\n",
      "./model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38de4454",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/config.json s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/config.json\n",
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer_config.json\n",
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/special_tokens_map.json\n",
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/generation_config.json s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/generation_config.json\n",
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/pytorch_model.bin.index.json s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model.bin.index.json\n",
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/tokenizer.model s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer.model\n",
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/pytorch_model-00002-of-00002.bin s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00002-of-00002.bin\n",
      "cp model/models--pinkmanlove--llama-7b-hf/snapshots/b3cde76468bad3c085ead29707ee7481121a4ca0/pytorch_model-00001-of-00002.bin s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00001-of-00002.bin\n"
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama/pretrain/pinkmanlove/llama-7b-hf/ \n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d498",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2057f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "# RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\n",
    "\n",
    "RUN pip3 uninstall -y deepspeed && pip3 install deepspeed\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8ee553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b0fb6",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53800617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-vicuna-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d98c7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  63.31MB\n",
      "Step 1/6 : From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      "1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04: Pulling from huggingface-pytorch-training\n",
      "846c0b181fff: Pulling fs layer\n",
      "6599ec13b57f: Pulling fs layer\n",
      "e1426114a55b: Pulling fs layer\n",
      "f5a45ff4f0b5: Pulling fs layer\n",
      "51873ca0db79: Pulling fs layer\n",
      "ba85a1106930: Pulling fs layer\n",
      "f0274a61d84a: Pulling fs layer\n",
      "5fef4e160b92: Pulling fs layer\n",
      "b2e49bf52c10: Pulling fs layer\n",
      "6208b54ab38f: Pulling fs layer\n",
      "5eee662f8ffb: Pulling fs layer\n",
      "5c2de35f7802: Pulling fs layer\n",
      "49e65772adc2: Pulling fs layer\n",
      "51b997129325: Pulling fs layer\n",
      "4f3d49a93e44: Pulling fs layer\n",
      "64c7d90d5392: Pulling fs layer\n",
      "ba85a1106930: Waiting\n",
      "4c9713f0d0ef: Pulling fs layer\n",
      "a3fbbed275fe: Pulling fs layer\n",
      "f0274a61d84a: Waiting\n",
      "f741bfc9a1c7: Pulling fs layer\n",
      "5fef4e160b92: Waiting\n",
      "095de69a84e1: Pulling fs layer\n",
      "1f7e19282619: Pulling fs layer\n",
      "2c4bc2d016c8: Pulling fs layer\n",
      "b53c636a5e48: Pulling fs layer\n",
      "b1acddac4706: Pulling fs layer\n",
      "88ddc4396001: Pulling fs layer\n",
      "b2e49bf52c10: Waiting\n",
      "c521fa337482: Pulling fs layer\n",
      "49e65772adc2: Waiting\n",
      "6208b54ab38f: Waiting\n",
      "1f7e19282619: Waiting\n",
      "2c4bc2d016c8: Waiting\n",
      "5eee662f8ffb: Waiting\n",
      "64c7d90d5392: Waiting\n",
      "4c9713f0d0ef: Waiting\n",
      "a3fbbed275fe: Waiting\n",
      "f741bfc9a1c7: Waiting\n",
      "5c2de35f7802: Waiting\n",
      "095de69a84e1: Waiting\n",
      "51b997129325: Waiting\n",
      "b1acddac4706: Waiting\n",
      "4f3d49a93e44: Waiting\n",
      "f5a45ff4f0b5: Waiting\n",
      "88ddc4396001: Waiting\n",
      "b53c636a5e48: Waiting\n",
      "51873ca0db79: Waiting\n",
      "8c67c55aef67: Pulling fs layer\n",
      "c521fa337482: Waiting\n",
      "fd1d36e694f5: Pulling fs layer\n",
      "4a30d66d1524: Pulling fs layer\n",
      "8c67c55aef67: Waiting\n",
      "d7aa1ef53c7e: Pulling fs layer\n",
      "fd1d36e694f5: Waiting\n",
      "bd51662b3565: Pulling fs layer\n",
      "da4aa42827f3: Pulling fs layer\n",
      "4a30d66d1524: Waiting\n",
      "11d6d4278e81: Pulling fs layer\n",
      "d7aa1ef53c7e: Waiting\n",
      "bd51662b3565: Waiting\n",
      "da4aa42827f3: Waiting\n",
      "11d6d4278e81: Waiting\n",
      "b1aa8711ab70: Pulling fs layer\n",
      "6e6fcb42ee70: Pulling fs layer\n",
      "868516bfc971: Pulling fs layer\n",
      "d70a085e7e61: Pulling fs layer\n",
      "40023a1eb6ce: Pulling fs layer\n",
      "ee52a13b955a: Pulling fs layer\n",
      "73989d76116b: Pulling fs layer\n",
      "cb2464d6294c: Pulling fs layer\n",
      "2e0a53d22c96: Pulling fs layer\n",
      "e7419902bf07: Pulling fs layer\n",
      "6d1e693998ac: Pulling fs layer\n",
      "798a4ccd4ada: Pulling fs layer\n",
      "72dc3d44cd43: Pulling fs layer\n",
      "2abe3baa9715: Pulling fs layer\n",
      "33b742d00444: Pulling fs layer\n",
      "9c9c433fe42e: Pulling fs layer\n",
      "2abe3baa9715: Waiting\n",
      "e7419902bf07: Waiting\n",
      "d70a085e7e61: Waiting\n",
      "6d1e693998ac: Waiting\n",
      "40023a1eb6ce: Waiting\n",
      "9c9c433fe42e: Waiting\n",
      "cb2464d6294c: Waiting\n",
      "33b742d00444: Waiting\n",
      "798a4ccd4ada: Waiting\n",
      "6e6fcb42ee70: Waiting\n",
      "2e0a53d22c96: Waiting\n",
      "73989d76116b: Waiting\n",
      "ee52a13b955a: Waiting\n",
      "868516bfc971: Waiting\n",
      "6599ec13b57f: Verifying Checksum\n",
      "6599ec13b57f: Download complete\n",
      "f5a45ff4f0b5: Verifying Checksum\n",
      "f5a45ff4f0b5: Download complete\n",
      "51873ca0db79: Verifying Checksum\n",
      "51873ca0db79: Download complete\n",
      "846c0b181fff: Download complete\n",
      "ba85a1106930: Verifying Checksum\n",
      "ba85a1106930: Download complete\n",
      "e1426114a55b: Verifying Checksum\n",
      "e1426114a55b: Download complete\n",
      "846c0b181fff: Pull complete\n",
      "b2e49bf52c10: Verifying Checksum\n",
      "b2e49bf52c10: Download complete\n",
      "6208b54ab38f: Verifying Checksum\n",
      "6208b54ab38f: Download complete\n",
      "6599ec13b57f: Pull complete\n",
      "e1426114a55b: Pull complete\n",
      "f5a45ff4f0b5: Pull complete\n",
      "51873ca0db79: Pull complete\n",
      "ba85a1106930: Pull complete\n",
      "5fef4e160b92: Verifying Checksum\n",
      "5fef4e160b92: Download complete\n",
      "5c2de35f7802: Verifying Checksum\n",
      "5c2de35f7802: Download complete\n",
      "49e65772adc2: Verifying Checksum\n",
      "49e65772adc2: Download complete\n",
      "51b997129325: Download complete\n",
      "4f3d49a93e44: Verifying Checksum\n",
      "4f3d49a93e44: Download complete\n",
      "64c7d90d5392: Verifying Checksum\n",
      "64c7d90d5392: Download complete\n",
      "4c9713f0d0ef: Verifying Checksum\n",
      "4c9713f0d0ef: Download complete\n",
      "a3fbbed275fe: Verifying Checksum\n",
      "a3fbbed275fe: Download complete\n",
      "5eee662f8ffb: Verifying Checksum\n",
      "5eee662f8ffb: Download complete\n",
      "095de69a84e1: Verifying Checksum\n",
      "095de69a84e1: Download complete\n",
      "1f7e19282619: Verifying Checksum\n",
      "1f7e19282619: Download complete\n",
      "2c4bc2d016c8: Verifying Checksum\n",
      "2c4bc2d016c8: Download complete\n",
      "b53c636a5e48: Download complete\n",
      "b1acddac4706: Verifying Checksum\n",
      "b1acddac4706: Download complete\n",
      "88ddc4396001: Download complete\n",
      "c521fa337482: Verifying Checksum\n",
      "c521fa337482: Download complete\n",
      "8c67c55aef67: Verifying Checksum\n",
      "8c67c55aef67: Download complete\n",
      "fd1d36e694f5: Verifying Checksum\n",
      "fd1d36e694f5: Download complete\n",
      "4a30d66d1524: Verifying Checksum\n",
      "4a30d66d1524: Download complete\n",
      "d7aa1ef53c7e: Verifying Checksum\n",
      "d7aa1ef53c7e: Download complete\n",
      "bd51662b3565: Verifying Checksum\n",
      "bd51662b3565: Download complete\n",
      "da4aa42827f3: Download complete\n",
      "11d6d4278e81: Verifying Checksum\n",
      "11d6d4278e81: Download complete\n",
      "b1aa8711ab70: Verifying Checksum\n",
      "b1aa8711ab70: Download complete\n",
      "6e6fcb42ee70: Verifying Checksum\n",
      "6e6fcb42ee70: Download complete\n",
      "868516bfc971: Verifying Checksum\n",
      "868516bfc971: Download complete\n",
      "d70a085e7e61: Verifying Checksum\n",
      "d70a085e7e61: Download complete\n",
      "40023a1eb6ce: Verifying Checksum\n",
      "40023a1eb6ce: Download complete\n",
      "ee52a13b955a: Download complete\n",
      "73989d76116b: Verifying Checksum\n",
      "73989d76116b: Download complete\n",
      "cb2464d6294c: Verifying Checksum\n",
      "cb2464d6294c: Download complete\n",
      "2e0a53d22c96: Verifying Checksum\n",
      "2e0a53d22c96: Download complete\n",
      "e7419902bf07: Verifying Checksum\n",
      "e7419902bf07: Download complete\n",
      "6d1e693998ac: Verifying Checksum\n",
      "6d1e693998ac: Download complete\n",
      "798a4ccd4ada: Verifying Checksum\n",
      "798a4ccd4ada: Download complete\n",
      "72dc3d44cd43: Verifying Checksum\n",
      "72dc3d44cd43: Download complete\n",
      "2abe3baa9715: Download complete\n",
      "33b742d00444: Download complete\n",
      "9c9c433fe42e: Verifying Checksum\n",
      "9c9c433fe42e: Download complete\n",
      "f741bfc9a1c7: Verifying Checksum\n",
      "f741bfc9a1c7: Download complete\n",
      "f0274a61d84a: Verifying Checksum\n",
      "f0274a61d84a: Download complete\n",
      "f0274a61d84a: Pull complete\n",
      "5fef4e160b92: Pull complete\n",
      "b2e49bf52c10: Pull complete\n",
      "6208b54ab38f: Pull complete\n",
      "5eee662f8ffb: Pull complete\n",
      "5c2de35f7802: Pull complete\n",
      "49e65772adc2: Pull complete\n",
      "51b997129325: Pull complete\n",
      "4f3d49a93e44: Pull complete\n",
      "64c7d90d5392: Pull complete\n",
      "4c9713f0d0ef: Pull complete\n",
      "a3fbbed275fe: Pull complete\n",
      "f741bfc9a1c7: Pull complete\n",
      "095de69a84e1: Pull complete\n",
      "1f7e19282619: Pull complete\n",
      "2c4bc2d016c8: Pull complete\n",
      "b53c636a5e48: Pull complete\n",
      "b1acddac4706: Pull complete\n",
      "88ddc4396001: Pull complete\n",
      "c521fa337482: Pull complete\n",
      "8c67c55aef67: Pull complete\n",
      "fd1d36e694f5: Pull complete\n",
      "4a30d66d1524: Pull complete\n",
      "d7aa1ef53c7e: Pull complete\n",
      "bd51662b3565: Pull complete\n",
      "da4aa42827f3: Pull complete\n",
      "11d6d4278e81: Pull complete\n",
      "b1aa8711ab70: Pull complete\n",
      "6e6fcb42ee70: Pull complete\n",
      "868516bfc971: Pull complete\n",
      "d70a085e7e61: Pull complete\n",
      "40023a1eb6ce: Pull complete\n",
      "ee52a13b955a: Pull complete\n",
      "73989d76116b: Pull complete\n",
      "cb2464d6294c: Pull complete\n",
      "2e0a53d22c96: Pull complete\n",
      "e7419902bf07: Pull complete\n",
      "6d1e693998ac: Pull complete\n",
      "798a4ccd4ada: Pull complete\n",
      "72dc3d44cd43: Pull complete\n",
      "2abe3baa9715: Pull complete\n",
      "33b742d00444: Pull complete\n",
      "9c9c433fe42e: Pull complete\n",
      "Digest: sha256:6465c5dd6672419b1a60cb47dab82a0f4f1cca22abe3ba7ed9af0c313836df26\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      " ---> c5a6ef695006\n",
      "Step 2/6 : ENV LANG=C.UTF-8\n",
      " ---> Running in 689e62174862\n",
      "Removing intermediate container 689e62174862\n",
      " ---> f895e7d7cc1e\n",
      "Step 3/6 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in fcc5b731a02e\n",
      "Removing intermediate container fcc5b731a02e\n",
      " ---> 66fc6dd72095\n",
      "Step 4/6 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Running in ce01aa26204f\n",
      "Removing intermediate container ce01aa26204f\n",
      " ---> 785f3704dbb7\n",
      "Step 5/6 : RUN pip3 uninstall -y deepspeed && pip3 install deepspeed\n",
      " ---> Running in 7abb96ff869e\n",
      "Found existing installation: deepspeed 0.6.1+06f2048\n",
      "Uninstalling deepspeed-0.6.1+06f2048:\n",
      "  Successfully uninstalled deepspeed-0.6.1+06f2048\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mCollecting deepspeed\n",
      "  Downloading deepspeed-0.9.2.tar.gz (779 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 779.3/779.3 kB 18.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.11.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed) (5.9.4)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.10.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.13.1+cu117)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from deepspeed) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<2.0.0->deepspeed) (4.4.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py): started\n",
      "  Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.9.2-py3-none-any.whl size=811192 sha256=e46f6a6c10d648ec1e7c7af478ea738d595bc0142205b6d687801f99439bd6ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/86/f9/5b6341574584972377c6d55ff5e8fa53c956a39d63a849ffb4\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: deepspeed\n",
      "Successfully installed deepspeed-0.9.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 7abb96ff869e\n",
      " ---> a0f651ef276a\n",
      "Step 6/6 : ENV NVIDIA_VISIBLE_DEVICES=\"all\"\n",
      " ---> Running in 47ea8dbbdfee\n",
      "Removing intermediate container 47ea8dbbdfee\n",
      " ---> 73a5ed858cfa\n",
      "Successfully built 73a5ed858cfa\n",
      "Successfully tagged sagemaker-vicuna-demo:latest\n",
      "The push refers to repository [687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-vicuna-demo]\n",
      "39f033aca837: Preparing\n",
      "f8dae5c3df1e: Preparing\n",
      "e3221f18601a: Preparing\n",
      "b6f286626882: Preparing\n",
      "76fe97d80cdb: Preparing\n",
      "f5f76489fff8: Preparing\n",
      "621c3f07daa7: Preparing\n",
      "9b484bb42e11: Preparing\n",
      "54c7c0b58471: Preparing\n",
      "c34adc3ab668: Preparing\n",
      "bbf651e48b84: Preparing\n",
      "f61045791108: Preparing\n",
      "4e2ac0cda74a: Preparing\n",
      "658a33d555eb: Preparing\n",
      "bd16d9a61a98: Preparing\n",
      "f0c0cd2accfa: Preparing\n",
      "1275469c066c: Preparing\n",
      "b802dd3babf4: Preparing\n",
      "621c3f07daa7: Waiting\n",
      "a3834ec63558: Preparing\n",
      "63edcef6dedf: Preparing\n",
      "9b484bb42e11: Waiting\n",
      "0154e84cc2dd: Preparing\n",
      "7085d1c151f6: Preparing\n",
      "54c7c0b58471: Waiting\n",
      "a77a2104cfb6: Preparing\n",
      "6808e7f9da2f: Preparing\n",
      "c34adc3ab668: Waiting\n",
      "3bc059a9dec6: Preparing\n",
      "de783f3fec23: Preparing\n",
      "18ca52d74b2f: Preparing\n",
      "73df6ccd636c: Preparing\n",
      "4e2ac0cda74a: Waiting\n",
      "bbf651e48b84: Waiting\n",
      "6738b73ff7a8: Preparing\n",
      "2a8292d9bfcc: Preparing\n",
      "f61045791108: Waiting\n",
      "5b75a5ef32a7: Preparing\n",
      "25a5f55a11f0: Preparing\n",
      "f5f76489fff8: Waiting\n",
      "707f484816ae: Preparing\n",
      "658a33d555eb: Waiting\n",
      "0430aa1e47d4: Preparing\n",
      "65448e793131: Preparing\n",
      "6808e7f9da2f: Waiting\n",
      "a77a2104cfb6: Waiting\n",
      "15af6e2d42ba: Preparing\n",
      "18ca52d74b2f: Waiting\n",
      "b46caef92993: Preparing\n",
      "73df6ccd636c: Waiting\n",
      "bd16d9a61a98: Waiting\n",
      "53ce33a12646: Preparing\n",
      "6738b73ff7a8: Waiting\n",
      "aad68760f4ce: Preparing\n",
      "3bc059a9dec6: Waiting\n",
      "a3834ec63558: Waiting\n",
      "f0c0cd2accfa: Waiting\n",
      "323d67ab1719: Preparing\n",
      "2a8292d9bfcc: Waiting\n",
      "1275469c066c: Waiting\n",
      "e72743a0fdfe: Preparing\n",
      "63edcef6dedf: Waiting\n",
      "3996353f5820: Preparing\n",
      "5b75a5ef32a7: Waiting\n",
      "15af6e2d42ba: Waiting\n",
      "de783f3fec23: Waiting\n",
      "7085d1c151f6: Waiting\n",
      "ea87e0b9c30f: Preparing\n",
      "b46caef92993: Waiting\n",
      "25a5f55a11f0: Waiting\n",
      "af18356cdf10: Preparing\n",
      "65448e793131: Waiting\n",
      "707f484816ae: Waiting\n",
      "f6e30dd4497e: Preparing\n",
      "0430aa1e47d4: Waiting\n",
      "e72743a0fdfe: Waiting\n",
      "99832d04a153: Preparing\n",
      "aad68760f4ce: Waiting\n",
      "af18356cdf10: Waiting\n",
      "a5981ed7a378: Preparing\n",
      "53ce33a12646: Waiting\n",
      "f6e30dd4497e: Waiting\n",
      "ea87e0b9c30f: Waiting\n",
      "250519a2f830: Preparing\n",
      "6cadbde53f94: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "250519a2f830: Waiting\n",
      "6cadbde53f94: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "f8dae5c3df1e: Pushed\n",
      "f5f76489fff8: Pushed\n",
      "e3221f18601a: Pushed\n",
      "39f033aca837: Pushed\n",
      "621c3f07daa7: Pushed\n",
      "9b484bb42e11: Pushed\n",
      "bbf651e48b84: Pushed\n",
      "c34adc3ab668: Pushed\n",
      "f61045791108: Pushed\n",
      "b6f286626882: Pushed\n",
      "4e2ac0cda74a: Pushed\n",
      "bd16d9a61a98: Pushed\n",
      "1275469c066c: Pushed\n",
      "b802dd3babf4: Pushed\n",
      "a3834ec63558: Pushed\n",
      "63edcef6dedf: Pushed\n",
      "54c7c0b58471: Pushed\n",
      "76fe97d80cdb: Pushed\n",
      "a77a2104cfb6: Pushed\n",
      "7085d1c151f6: Pushed\n",
      "3bc059a9dec6: Pushed\n",
      "de783f3fec23: Pushed\n",
      "0154e84cc2dd: Pushed\n",
      "73df6ccd636c: Pushed\n",
      "18ca52d74b2f: Pushed\n",
      "2a8292d9bfcc: Pushed\n",
      "6738b73ff7a8: Pushed\n",
      "5b75a5ef32a7: Pushed\n",
      "707f484816ae: Pushed\n",
      "f0c0cd2accfa: Pushed\n",
      "0430aa1e47d4: Pushed\n",
      "65448e793131: Pushed\n",
      "15af6e2d42ba: Pushed\n",
      "53ce33a12646: Pushed\n",
      "aad68760f4ce: Pushed\n",
      "6808e7f9da2f: Pushed\n",
      "e72743a0fdfe: Pushed\n",
      "658a33d555eb: Pushed\n",
      "3996353f5820: Pushed\n",
      "b46caef92993: Pushed\n",
      "f6e30dd4497e: Pushed\n",
      "99832d04a153: Pushed\n",
      "a5981ed7a378: Pushed\n",
      "250519a2f830: Pushed\n",
      "6cadbde53f94: Pushed\n",
      "0002c93bdb37: Pushed\n",
      "ea87e0b9c30f: Pushed\n",
      "25a5f55a11f0: Pushed\n",
      "323d67ab1719: Pushed\n",
      "af18356cdf10: Pushed\n",
      "latest: digest: sha256:1aa44af892e4216fe884fe87bdebedeb7221b54d48e1d453b55809d4857b2145 size: 10832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6dc0d",
   "metadata": {},
   "source": [
    "### Generate the deepspeed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7b56758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ds.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ds.json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"auto_cast\": false,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": \"auto\",\n",
    "      \"betas\": \"auto\",\n",
    "      \"eps\": \"auto\",\n",
    "      \"weight_decay\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": \"auto\",\n",
    "      \"warmup_max_lr\": \"auto\",\n",
    "      \"warmup_num_steps\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"sub_group_size\": 1e9,\n",
    "    \"reduce_bucket_size\": \"auto\",\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5fc567",
   "metadata": {},
   "source": [
    "**Generate training entrypoint script.**\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e3d70",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/7B/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3549fa81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export PYTHONPATH=./FastChat:$PYTHONPATH\n",
    "# echo $PYTHONPATH\n",
    "# cd FastChat && pip install -e . && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01a1a6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile train.sh\n",
    "# #!/bin/bash\n",
    "\n",
    "# chmod +x ./s5cmd\n",
    "# ./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "\n",
    "# cd FastChat && pip install -e . && cd ..\n",
    "\n",
    "# torchrun --nproc_per_node=8 --master_port=20001 FastChat/fastchat/train/train_mem.py \\\n",
    "#     --model_name_or_path /tmp/llama_pretrain/  \\\n",
    "#     --data_path sharegpt_test.json \\\n",
    "#     --bf16 True \\\n",
    "#     --output_dir /tmp/llama_out \\\n",
    "#     --num_train_epochs 3 \\\n",
    "#     --per_device_train_batch_size 2 \\\n",
    "#     --per_device_eval_batch_size 2 \\\n",
    "#     --gradient_accumulation_steps 16 \\\n",
    "#     --evaluation_strategy \"no\" \\\n",
    "#     --save_strategy \"steps\" \\\n",
    "#     --save_steps 1200 \\\n",
    "#     --save_total_limit 10 \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --lr_scheduler_type \"cosine\" \\\n",
    "#     --logging_steps 1 \\\n",
    "#     --fsdp \"full_shard auto_wrap\" \\\n",
    "#     --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n",
    "#     --tf32 True \\\n",
    "#     --model_max_length 2048 \\\n",
    "#     --gradient_checkpointing True \\\n",
    "#     --lazy_preprocess True \\\n",
    "#     --report_to \"none\"\n",
    "\n",
    "# if [ $? -eq 1 ]; then\n",
    "#     echo \"Training script error, please check CloudWatch logs\"\n",
    "#     exit 1\n",
    "# fi\n",
    "\n",
    "# ./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llama/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c6fa5",
   "metadata": {},
   "source": [
    "We will use deepspeed launch instead of torch run.\n",
    "\n",
    "(When use torch run, there will report tensor size not match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c51767",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "We modified some parts of ```FastChat/fastchat/train/train.py```, such as how to save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f94daee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv FastChat/fastchat/train/train.py FastChat/fastchat/train/train_bak.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "286ccd3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FastChat/fastchat/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FastChat/fastchat/train/train.py\n",
    "# Adopted from tatsu-lab@stanford_alpaca. Below is the original copyright:\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "####\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "####\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import get_conv_template, SeparatorStyle\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {\n",
    "            key: value.cpu()\n",
    "            for key, value in state_dict.items()\n",
    "        }\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conv_template(\"vicuna_v1.1\").copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            round_len = len(tokenizer(rou).input_ids)\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len:cur_len+instruction_len] = (\n",
    "                IGNORE_TOKEN_ID)\n",
    "\n",
    "            #rank0_print(tokenizer.decode(target[cur_len+instruction_len:cur_len+round_len]))\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                rank0_print(f\"WARNING: tokenization mismatch \"\n",
    "                            f\"{cur_len} vs. {total_len}\")\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets,\n",
    "                attention_mask=input_ids.ne(tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        rank0_print(\"Loading data...\")\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in list_data_dict]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i],\n",
    "                    labels=self.labels[i],\n",
    "                    attention_mask=self.attention_mask[i])\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Loading data...\")\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.list_data_dict = list_data_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        data_dict = preprocess([e[\"conversations\"] for e in sources],\n",
    "            self.tokenizer)\n",
    "        if isinstance(i, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                             labels=data_dict[\"labels\"][0],\n",
    "                             attention_mask=data_dict[\"attention_mask\"][0])\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (LazySupervisedDataset\n",
    "                   if data_args.lazy_preprocess else SupervisedDataset)\n",
    "    train_dataset = dataset_cls(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path)\n",
    "    return dict(train_dataset=train_dataset,\n",
    "                eval_dataset=None)\n",
    "\n",
    "\n",
    "def train():\n",
    "    global local_rank\n",
    "\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    local_rank = training_args.local_rank\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained( #transformers.AutoTokenizer\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "#####\n",
    "#     tokenizer.pad_token = tokenizer.unk_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"-----------no pad token and add special token PAD----\")\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "######\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                              data_args=data_args)\n",
    "    trainer = Trainer(model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      args=training_args,\n",
    "                      **data_module)\n",
    "\n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        trainer.train()\n",
    "#     trainer.save_state()\n",
    "#     safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "#                                    output_dir=training_args.output_dir)\n",
    "\n",
    "\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef55e2",
   "metadata": {},
   "source": [
    "Here we use sample dataset - sharegpt_test.json for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55cedcb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ds-train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ds-train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "\n",
    "cd FastChat && pip install -e . && cd ..\n",
    "\n",
    "\n",
    "deepspeed --num_gpus=8 FastChat/fastchat/train/train_mem.py \\\n",
    "    --deepspeed ds.json \\\n",
    "    --model_name_or_path \"/tmp/llama_pretrain/\" \\\n",
    "    --data_path sharegpt_test.json \\\n",
    "    --output_dir \"/tmp/llama_out\" \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size  2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --save_steps 2000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --cache_dir '/tmp' \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    --fp16 True \\\n",
    "    --tf32 True \\\n",
    "    --report_to \"none\"\n",
    "\n",
    "if [ $? -eq 1 ]; then\n",
    "    echo \"Training script error, please check CloudWatch logs\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llama/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "517889b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-vicuna-demo:latest'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad795754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "train_data_path = f's3://{sagemaker_default_bucket}/llama/train_data/'\n",
    "\n",
    "inputs = {'train': train_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ab36100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: vicuna-demo-2023-05-10-12-33-22-401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-10 12:33:26 Starting - Starting the training job......\n",
      "2023-05-10 12:34:15 Starting - Preparing the instances for training.........\n",
      "2023-05-10 12:35:56 Downloading - Downloading input data\n",
      "2023-05-10 12:35:56 Training - Downloading the training image.....................\n",
      "2023-05-10 12:39:22 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:21,944 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:22,006 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:22,015 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:22,017 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:22,970 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:23,043 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:23,117 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:23,127 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"vicuna-demo-2023-05-10-12-33-22-401\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-05-10-12-33-22-401/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds-train.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds-train.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds-train.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds-train.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-05-10-12-33-22-401/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"vicuna-demo-2023-05-10-12-33-22-401\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-05-10-12-33-22-401/source/sourcedir.tar.gz\",\"module_name\":\"ds-train.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds-train.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./ds-train.sh \"\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:40:27.223: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:27,227 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-05-10 12:40:27,245 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00002-of-00002.bin /tmp/llama_pretrain/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00001-of-00002.bin /tmp/llama_pretrain/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code/FastChat\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting nh3\u001b[0m\n",
      "\u001b[34mDownloading nh3-0.2.11-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 30.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 109.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (3.0.36)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (12.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (0.13.2)\u001b[0m\n",
      "\u001b[34mCollecting gradio==3.23\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.8/15.8 MB 90.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (1.23.5)\u001b[0m\n",
      "\u001b[34mCollecting markdown2[all]\u001b[0m\n",
      "\u001b[34mDownloading markdown2-2.4.8-py2.py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (0.1.97)\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.3/75.3 kB 23.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.7) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers<4.29.0,>=4.28.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 121.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting shortuuid\u001b[0m\n",
      "\u001b[34mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 17.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.95.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 18.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting orjson\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.8.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.2/137.2 kB 30.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting semantic-version\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 24.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.13.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 49.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 15.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (2023.1.0)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 13.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting altair>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.0.0-py3-none-any.whl (477 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 477.4/477.4 kB 70.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.0.tar.gz (4.8 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.7) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting websockets>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 38.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.7) (0.2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.7) (0.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.7) (2.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.7) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.7) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.7) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.7) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate->fschat==0.2.7) (5.9.4)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.27.0,>=0.26.1\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.26.1-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 20.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.6/70.6 kB 21.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from httpx->fschat==0.2.7) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna in /opt/conda/lib/python3.9/site-packages (from httpx->fschat==0.2.7) (3.4)\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting wavedrom\u001b[0m\n",
      "\u001b[34mDownloading wavedrom-2.0.3.post3.tar.gz (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.7/137.7 kB 36.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.7) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.7) (1.26.14)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from uvicorn->fschat==0.2.7) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.7) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.7) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.7) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.31-py3-none-any.whl (184 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 40.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.22.2-py2.py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.3/203.3 kB 48.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio==3.23->fschat==0.2.7) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio==3.23->fschat==0.2.7) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->fschat==0.2.7) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 19.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 26.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio==3.23->fschat==0.2.7) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->gradio==3.23->fschat==0.2.7) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio==3.23->fschat==0.2.7) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio==3.23->fschat==0.2.7) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio==3.23->fschat==0.2.7) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio==3.23->fschat==0.2.7) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio==3.23->fschat==0.2.7) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio==3.23->fschat==0.2.7) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio==3.23->fschat==0.2.7) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio==3.23->fschat==0.2.7) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio==3.23->fschat==0.2.7) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio==3.23->fschat==0.2.7) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->gradio==3.23->fschat==0.2.7) (0.11.0)\u001b[0m\n",
      "\u001b[34mCollecting svgwrite\u001b[0m\n",
      "\u001b[34mDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 18.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.23->fschat==0.2.7) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fschat, ffmpy, pathtools, wavedrom\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fschat: filename=fschat-0.2.7-0.editable-py3-none-any.whl size=12673 sha256=392ecb76d8ae1d11f120ea7fed0c1c1d730fc0e6a9eb95430b2255447bf36dad\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-d1myrz06/wheels/db/bc/89/1dbc1ea5766ee06de619a402c8ee5502723e161b730e182b5b\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=f0e6796dd2752b2ee569cfbb0364124c98effadef28c0b84fe1af780e8f8c5c2\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ec20c8283bbf906ff40152f1914e2bcb0303eaa90500b6b0c4ebf685a693f951\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=29933 sha256=b97d13a3f2dbc79e6f0b3daa218fe40ffc503b908b00bb2cfa4ac08c4dc1e2bb\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/81/08/ec/3e7bb60504c4ebf08e1d5c88e9abb85b0a3549d9f8d031113f\u001b[0m\n",
      "\u001b[34mSuccessfully built fschat ffmpy pathtools wavedrom\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, pathtools, nh3, ffmpy, websockets, uc-micro-py, svgwrite, sniffio, smmap, shortuuid, setproctitle, sentry-sdk, semantic-version, python-multipart, orjson, mdurl, markdown2, h11, docker-pycreds, aiofiles, wavedrom, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, gitdb, anyio, transformers, starlette, mdit-py-plugins, httpcore, GitPython, altair, wandb, httpx, fastapi, gradio, fschat\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.31 aiofiles-23.1.0 altair-5.0.0 anyio-3.6.2 docker-pycreds-0.4.0 fastapi-0.95.1 ffmpy-0.3.0 fschat-0.2.7 gitdb-4.0.10 gradio-3.23.0 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.14.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 markdown2-2.4.8 mdit-py-plugins-0.3.3 mdurl-0.1.2 nh3-0.2.11 orjson-3.8.12 pathtools-0.1.2 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 sentry-sdk-1.22.2 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 sniffio-1.3.0 starlette-0.26.1 svgwrite-1.4.3 transformers-4.28.1 uc-micro-py-1.0.2 uvicorn-0.22.0 wandb-0.15.2 wavedrom-2.0.3.post3 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:17,960] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:18,017] [INFO] [runner.py:541:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None FastChat/fastchat/train/train_mem.py --deepspeed ds.json --model_name_or_path /tmp/llama_pretrain/ --data_path sharegpt_test.json --output_dir /tmp/llama_out --num_train_epochs 1 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --save_steps 2000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --cache_dir /tmp --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True --fp16 True --tf32 True --report_to none\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:222:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:222:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:222:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:222:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:247:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:20,767] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:25,784] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:41:34,836] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 6.74B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.39s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.86s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.48s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.38s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.85s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading data...\u001b[0m\n",
      "\u001b[34mFormatting inputs...Skip in lazy mode\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.09790849685669 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.051276922225952 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.046295404434204 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.043015480041504 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.050082206726074 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.04975724220276 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.048391819000244 seconds\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 24.04913306236267 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.702630281448364 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.533015251159668 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.732379674911499 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.727993965148926 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.727819204330444 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.732184410095215 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.731356382369995 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.732446908950806 seconds\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...Loading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00038933753967285156 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003705024719238281 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004334449768066406 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003974437713623047 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004024505615234375 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00038242340087890625 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003764629364013672 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.703: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.703: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.706: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.706: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.706: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.706: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.706: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.729 algo-1:352 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.729 algo-1:348 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.733 algo-1:349 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.733 algo-1:346 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.733 algo-1:351 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.734 algo-1:350 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.734 algo-1:347 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00037598609924316406 seconds\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.748 algo-1:352 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.748 algo-1:348 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.752 algo-1:349 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.752 algo-1:346 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.752 algo-1:351 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.753 algo-1:350 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.753 algo-1:347 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.831: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.858 algo-1:345 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:33.876 algo-1:345 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 1598 vs. 1601\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 0.1181, 'learning_rate': 0, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2.4913, 'train_samples_per_second': 3.211, 'train_steps_per_second': 0.401, 'train_loss': 0.1180572509765625, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:47,899] [INFO] [launch.py:460:main] Process 348 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:47,899] [INFO] [launch.py:460:main] Process 349 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:47,899] [INFO] [launch.py:460:main] Process 346 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:47,900] [INFO] [launch.py:460:main] Process 351 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:48,901] [INFO] [launch.py:460:main] Process 350 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:48,901] [INFO] [launch.py:460:main] Process 352 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:42:48,901] [INFO] [launch.py:460:main] Process 347 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-05-10 12:43:00,915] [INFO] [launch.py:460:main] Process 345 exits successfully.\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/added_tokens.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/generation_config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/training_args.bin s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer.model s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/pytorch_model.bin s3://sagemaker-us-west-2-687912291502/llama/output/2023-05-10-12-43-02/llama_out/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2023-05-10 12:43:44,925 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-10 12:43:44,925 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-10 12:43:44,925 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-10 12:43:54 Uploading - Uploading generated training model\n",
      "2023-05-10 12:43:54 Completed - Training job completed\n",
      "Training seconds: 493\n",
      "Billable seconds: 493\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'vicuna-demo'         \n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train.sh',\n",
    "                      source_dir='./',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "estimator.fit()\n",
    "#estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e673c",
   "metadata": {},
   "source": [
    "You could find the model path in S3 from above logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b535a9-a683-473b-a769-8d3b3354d89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
